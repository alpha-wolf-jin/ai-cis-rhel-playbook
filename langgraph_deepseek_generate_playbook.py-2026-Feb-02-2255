#!/usr/bin/env python3
"""
Generic Ansible Playbook Generator (LangGraph Version)

This script wraps the existing playbook generator with LangGraph for better
state management, retry logic, and workflow visualization.

The interface (inputs, outputs, and main functions) remains exactly the same
as deepseek_generate_playbook.py for compatibility.
"""

import os
import sys, re
from typing import TypedDict, Literal
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langgraph.graph import StateGraph, END

# Import all functions from the original module
from deepseek_generate_playbook import (
    generate_playbook as _original_generate_playbook,
    save_playbook,
    check_playbook_syntax,
    test_playbook_on_server,
    analyze_playbook_output,  # New analysis function
    extract_playbook_issues_from_analysis,  # Helper to detect issues/advice
    # Keep argparse and main separate for now
)

# Load environment variables
load_dotenv()

## Initialize LLM model
#model = ChatDeepSeek(
#    model="deepseek-chat",
#    temperature=0,
#    max_tokens=None,
#    timeout=None,
#    max_retries=3
#)

# Define State for LangGraph workflow
class PlaybookGenerationState(TypedDict):
    """State for the playbook generation workflow."""
    # Input parameters
    playbook_objective: str
    target_host: str
    test_host: str
    become_user: str
    requirements: list[str]
    example_output: str
    filename: str
    max_retries: int
    audit_procedure: str  # CIS Benchmark audit procedure
    
    # Workflow state
    attempt: int
    playbook_content: str
    syntax_valid: bool
    test_success: bool
    analysis_passed: bool  # New field for analysis result
    analysis_message: str  # New field for analysis message
    final_success: bool
    error_message: str
    test_output: str
    final_output: str
    connection_error: bool  # Flag for connection errors (cannot validate on host)
    
    # Control flow
    should_retry: bool
    workflow_complete: bool


def generate_playbook_node(state: PlaybookGenerationState) -> PlaybookGenerationState:
    """LangGraph node: Generate or enhance playbook using LLM."""
    is_enhancement = state.get('playbook_content') and state.get('analysis_message')
    
    print(f"\n{'='*80} generate_playbook_node")
    if is_enhancement:
        print(f"Attempt {state['attempt']}/{state['max_retries']}: Enhancing Ansible playbook...")
    else:
        print(f"Attempt {state['attempt']}/{state['max_retries']}: Generating Ansible playbook...")
    print("=" * 80)
    print(f"Objective: {state['playbook_objective']}")
    print(f"Test Host: {state['test_host']}")
    if state['test_host'] != state['target_host']:
        print(f"Target Host: {state['target_host']}")
    print(f"Become User: {state['become_user']}")
    print(f"Requirements: {len(state['requirements'])} items")
    if is_enhancement:
        print(f"Mode: Enhancement (based on existing playbook and feedback)")
    print("=" * 80)
    
    try:
        # Extract feedback from analysis_message if available
        feedback_content = None
        if state.get('analysis_message'):
            # Extract the PLAYBOOK ANALYSIS section and recommendations
            analysis_msg = state['analysis_message']
            from deepseek_generate_playbook import extract_playbook_issues_from_analysis
            has_issues, extracted_advice = extract_playbook_issues_from_analysis(analysis_msg)
            if has_issues and extracted_advice:
                feedback_content = extracted_advice
            elif has_issues:
                # Extract PLAYBOOK ANALYSIS section
                lines = analysis_msg.split('\n')
                feedback_lines = []
                for i, line in enumerate(lines):
                    if "PLAYBOOK ANALYSIS" in line.upper():
                        feedback_lines.append(line)
                        # Get next 20-30 lines for context
                        for j in range(i+1, min(i+30, len(lines))):
                            if lines[j].strip().startswith('- **') and 'PLAYBOOK' not in lines[j].upper() and 'DATA COLLECTION' not in lines[j].upper():
                                break
                            feedback_lines.append(lines[j])
                        break
                feedback_content = '\n'.join(feedback_lines).strip()
        
        # Use the original generate_playbook function
        playbook = _original_generate_playbook(
            playbook_objective=state['playbook_objective'],
            target_host=state['test_host'],
            become_user=state['become_user'],
            requirements=state['requirements'],
            example_output=state['example_output'],
            audit_procedure=state.get('audit_procedure', ''),
            current_playbook=state.get('playbook_content'),  # Pass current playbook for enhancement
            feedback=feedback_content  # Pass feedback for enhancement
        )
        
        # Display the generated/enhanced playbook
        if is_enhancement:
            print("\nüìã Enhanced Ansible Playbook:")
        else:
            print("\nüìã Generated Ansible Playbook:")
        print("=" * 80)
        print(playbook)
        print("=" * 80)
        
        state['playbook_content'] = playbook
        state['error_message'] = ""
        
    except Exception as e:
        state['error_message'] = str(e)
        state['playbook_content'] = ""
        print(f"‚ùå Error generating playbook: {e}")
    
    return state


def increment_attempt_node(state: PlaybookGenerationState) -> PlaybookGenerationState:
    """LangGraph node: Increment attempt counter for retry."""
    state['attempt'] += 1
    print(f"\n{'='*80} increment_attempt_node")
    print(f"\nüîÑ Incrementing attempt counter: {state['attempt']}/{state['max_retries']}")
    return state


def save_playbook_node(state: PlaybookGenerationState) -> PlaybookGenerationState:
    """LangGraph node: Save playbook to file."""
    print(f"\n{'='*80} save_playbook_node")
    if state['playbook_content']:
        save_playbook(state['playbook_content'], state['filename'])
    return state


def check_syntax_node(state: PlaybookGenerationState) -> PlaybookGenerationState:
    """LangGraph node: Check playbook syntax."""
    is_valid, error_msg = check_playbook_syntax(state['filename'], state['test_host'])
    print(f"\n{'='*80} check_syntax_node")
    
    state['syntax_valid'] = is_valid
    if not is_valid:
        state['error_message'] = error_msg
        # Don't set should_retry here - let conditional edge decide
        
        if state['attempt'] < state['max_retries']:
            print(f"\n‚ö†Ô∏è  Syntax check failed on attempt {state['attempt']}/{state['max_retries']}")
            print("üîÑ Retrying with additional instructions to LLM...")
            print("\nüìã Error Summary:")
            error_lines = error_msg.split('\n')
            for line in error_lines[:10]:
                if line.strip():
                    print(f"   {line}")
            if len(error_lines) > 10:
                print(f"   ... ({len(error_lines) - 10} more lines)")
            
            # Add error context to requirements for next attempt
            error_msg_escaped = error_msg[:200].replace('{', '{{').replace('}', '}}')
            state['requirements'].append(f"IMPORTANT: Previous attempt had syntax error: {error_msg_escaped}")
    
    return state


def test_on_test_host_node(state: PlaybookGenerationState) -> PlaybookGenerationState:
    """LangGraph node: Test playbook on test host."""
    print("\n" + "=" * 80)
    print(f"\n{'='*80} test_on_test_host_node")
    print(f"‚úÖ Syntax Valid! Now testing on test host: {state['test_host']}...")
    print("=" * 80)
    
    # Execute on test host with debug tasks skipped for cleaner analysis
    test_success, test_output = test_playbook_on_server(
        state['filename'],
        state['test_host'],
        check_mode=False,
        verbose=True,
        skip_debug=True  # Skip debug tasks for cleaner output to analyze
    )
    
    state['test_success'] = test_success
    state['test_output'] = test_output
    
    # Check if it's a connection error (cannot validate on host)
    if not test_success and test_output.startswith("CONNECTION_ERROR:"):
        print("\n" + "=" * 80)
        print("‚ö†Ô∏è  WARNING: Cannot connect to test host for validation")
        print("=" * 80)
        print(f"\n‚ùå Connection Error Details:")
        print(f"   Host: {state['test_host']}")
        print(f"   Error: {test_output.replace('CONNECTION_ERROR: ', '')}")
        print(f"\n‚ö†Ô∏è  The playbook syntax is valid, but execution validation cannot be performed.")
        print(f"   Please ensure:")
        print(f"   1. The host {state['test_host']} is reachable")
        print(f"   2. SSH access is configured correctly")
        print(f"   3. Authentication credentials are valid")
        print(f"\n‚úÖ Playbook has been saved with valid syntax: {state['filename']}")
        print("=" * 80)
        # Set error message and mark as connection error (don't retry)
        state['error_message'] = test_output
        state['connection_error'] = True
        return state
    
    if test_success:
        print("\n" + "=" * 80)
        print(f"üéâ SUCCESS! Playbook validated on test host: {state['test_host']}!")
        print("=" * 80)
        print("\n‚úÖ Test Execution Summary:")
        print("   1. ‚úÖ Syntax check passed")
        print(f"   2. ‚úÖ Test execution passed on {state['test_host']}")
        print("   3. ‚úÖ All requirements verified")
        
        # Show test output
        print(f"\nüìã Full Test Execution Output from {state['test_host']}:")
        print("=" * 80)
        print(test_output)
        print("=" * 80)
    else:
        state['error_message'] = test_output
        # Don't set should_retry here - let conditional edge decide
        
        if state['attempt'] < state['max_retries']:
            print(f"\n‚ö†Ô∏è  Server test failed on attempt {state['attempt']}/{state['max_retries']}")
            print("üîÑ Retrying with test failure feedback to LLM...")
            
            # Add error feedback for retry
            test_output_escaped = test_output[:300].replace('{', '{{').replace('}', '}}')
            state['requirements'].append(f"IMPORTANT: Previous playbook failed testing: {test_output_escaped}")
    
    return state

def parse_compliance_report(text):
    requirements = []
    data_collection = []
    playbook_analysis = []
    compliance_status = []
    recommendation = []
    issue = []
    requirements_target_block = False
    data_collection_target_block = False
    playbook_analysis_target_block = False
    compliance_status_target_block = False
    recommendation_target_block = False
    issue_target_block = False
    lines = text.split('\n')

    for line in lines:
        # Check for Start Line: Contains "PLAYBOOK ANALYSIS" and "FAIL"
        if "Based on the collected data" in line:
            requirements_target_block = True
            continue  # Skip the start line itself if you only want the content between
        # Check for End Line: First line containing "COMPLIANCE STATUS" after start
        if requirements_target_block and "OVERALL ASSESSMENT" in line:
            requirements_target_block = False
            #break # Stop capturing
            print(f"{'-'*20} requirements {'-'*20}")
            print(requirements)
        if requirements_target_block:
            requirements.append(line)
            
        if "DATA COLLECTION" in line and ':' in line:
            data_collection_target_block = True
        if data_collection_target_block:
            # End Line: If the line is empty or just whitespace
            if not line.strip():
                data_collection_target_block = False
                print(f"{'-'*20} data_collection {'-'*20}")
                print(data_collection)
        if data_collection_target_block:
            data_collection.append(line)

        if "PLAYBOOK ANALYSIS" in line and ':' in line:
            playbook_analysis_target_block = True
        if playbook_analysis_target_block:
            # End Line: If the line is empty or just whitespace
            if not line.strip():
                playbook_analysis_target_block = False
                print(f"{'-'*20} playbook_analysis {'-'*20}")
                print(playbook_analysis)
        if playbook_analysis_target_block:
            playbook_analysis.append(line)

        if "COMPLIANCE STATUS" in line and ':' in line:
            compliance_status_target_block = True
        if compliance_status_target_block:
            # End Line: If the line is empty or just whitespace
            if not line.strip():
                compliance_status_target_block = False
                print(f"{'-'*20} compliance_status {'-'*20}")
                print(compliance_status)
        if compliance_status_target_block:
            compliance_status.append(line)

        if "RECOMMENDATION" in line and ':' in line:
            recommendation_target_block = True
        if recommendation_target_block:
            # End Line: If the line is empty or just whitespace
            if not line.strip():
                recommendation_target_block = False
                print(f"{'-'*20} recommendation {'-'*20}")
                print(recommendation)
        if recommendation_target_block:
            recommendation.append(line)

    return {
        "requirements": '\n'.join(requirements),
        "data_collection": '\n'.join(data_collection),
        "playbook_analysis": '\n'.join(playbook_analysis),
        "compliance_status": '\n'.join(compliance_status),
        "recommendation": '\n'.join(recommendation)
    }

def analyze_output_node(state: PlaybookGenerationState) -> PlaybookGenerationState:
    """LangGraph node: Analyze playbook output against requirements."""
    print(f"\n{'='*80} analyze_output_node")
    test_success, test_output = test_playbook_on_server(
        state['filename'],
        state['test_host'],
        check_mode=False,
        verbose=True,
        skip_debug=True  # Skip debug tasks for cleaner output to analyze
    )
    
    state['test_success'] = test_success
    state['test_output'] = test_output
    
    if test_success:
        analysis_passed, analysis_message = analyze_playbook_output(
            requirements=state['requirements'],
            playbook_objective=state['playbook_objective'],
            test_output=state['test_output'],
            audit_procedure=state.get('audit_procedure'),
            playbook_content=state.get('playbook_content')  # Pass playbook content for analysis
        )

        # Check for PLAYBOOK ANALYSIS: FAIL status
        has_issues, extracted_advice = extract_playbook_issues_from_analysis(analysis_message)
        
        # Verify status alignment between playbook output and AI analysis
        from deepseek_generate_playbook import verify_status_alignment, extract_analysis_statuses
        status_aligned, alignment_message = verify_status_alignment(state['test_output'], analysis_message)
        
        # Extract all required statuses from analysis
        analysis_statuses = extract_analysis_statuses(analysis_message)
        
        # Proceed to target execution only when ALL criteria are met:
        # 1. DATA COLLECTION: PASS
        # 2. PLAYBOOK ANALYSIS: PASS
        # 3. COMPLIANCE STATUS: COMPLIANT
        # 4. All requirement statuses align (PASS/FAIL = COMPLIANT/NON-COMPLIANT)
        # 5. Overall status aligns
        data_collection_pass = analysis_statuses.get('data_collection') == 'PASS'
        playbook_analysis_pass = analysis_statuses.get('playbook_analysis') == 'PASS'
        compliance_status_compliant = analysis_statuses.get('compliance_status') == 'COMPLIANT'
     
        #ok = input('Continue langgraph_deepseek_generate_playbook.py? (enter at 306):')
        #if not analysis_passed:
        #parsed_data = parse_compliance_report(analysis_message)
        #print(f"{'-'*20} requirements {'-'*20}")
        #print(parsed_data['requirements'])
        #print(f"{'-'*20} data_collection {'-'*20}")
        #print(parsed_data['data_collection'])
        #print(f"{'-'*20} playbook_analysis {'-'*20}")
        #print(parsed_data['playbook_analysis'])
        #print(f"{'-'*20} compliance_status {'-'*20}")
        #print(parsed_data['compliance_status'])
        #print(f"{'-'*20} recommendation {'-'*20}")
        #print(parsed_data['recommendation'])
        #print(f"{'-'*50}")

        #effective_analysis_passed = (
        #    data_collection_pass and
        #    playbook_analysis_pass and
        #    compliance_status_compliant
        #)
        
        #print(f"effective_analysis_passed: {effective_analysis_passed}")
        #ok = input('Continue langgraph_deepseek_generate_playbook.py? (enter at 323):')

        # Debug output - only show when the three main sections don't all pass
        # Skip debug output when DATA COLLECTION, PLAYBOOK ANALYSIS, and COMPLIANCE STATUS all pass
        all_main_sections_pass = (
            data_collection_pass and
            playbook_analysis_pass and
            compliance_status_compliant
        )
    

        #extracted_lines = []
        #if not playbook_analysis_pass:
        #    lines = analysis_message.split('\n')
        #    inside_target_block = False
        #    for line in lines:
        #        # Check for Start Line: Contains "PLAYBOOK ANALYSIS" and "FAIL"
        #        if "PLAYBOOK ANALYSIS" in line and "FAIL" in line:
        #            inside_target_block = True
        #            continue  # Skip the start line itself if you only want the content between
        #        
        #        # Check for End Line: First line containing "COMPLIANCE STATUS" after start
        #        if inside_target_block and "COMPLIANCE STATUS" in line:
        #            break # Stop capturing
        #            
        #        if inside_target_block:
        #            extracted_lines.append(line)

        #    if len(extracted_lines) > 0:
        #        inside_target_block = False
        #        for line in lines:
        #            # Check for Start Line: Contains "PLAYBOOK ANALYSIS" and "FAIL"
        #            if "RECOMMENDATION" in line and ':' in line:
        #                inside_target_block = True
        #                continue  # Skip the start line itself if you only want the content between
        #            
        #            if inside_target_block:
        #                # End Line: If the line is empty or just whitespace
        #                if not line.strip():
        #                    break

        #            if inside_target_block:
        #                extracted_lines.append(line)


        #if not all_main_sections_pass:
        #    print(f"\nüîç DEBUG: AI COMPLIANCE ANALYSIS status check:")
        #    print(f"   - DATA COLLECTION: {analysis_statuses.get('data_collection', 'NOT FOUND')}")
        #    print(f"   - PLAYBOOK ANALYSIS: {analysis_statuses.get('playbook_analysis', 'NOT FOUND')}")
        #    print(f"   - COMPLIANCE STATUS: {analysis_statuses.get('compliance_status', 'NOT FOUND')}")
        #    #print(f"   - Status Alignment: {'‚úì ALIGNED' if status_aligned else '‚úó MISALIGNED'}")
        #    #if not status_aligned:
        #    #    print(f"   - Alignment Issues: {alignment_message}")
        
        state['analysis_passed'] = all_main_sections_pass
        state['analysis_message'] = analysis_message
        
        #if not effective_analysis_passed:
        if not all_main_sections_pass:
            # Check which criteria failed
            failed_criteria = []
            if not data_collection_pass:
                failed_criteria.append("DATA COLLECTION: not PASS")
            if not playbook_analysis_pass:
                failed_criteria.append("PLAYBOOK ANALYSIS: not PASS")
            if not compliance_status_compliant:
                failed_criteria.append("COMPLIANCE STATUS: not COMPLIANT")
            if has_issues:
                failed_criteria.append("PLAYBOOK ANALYSIS: FAIL (has issues)")
            #if not status_aligned:
            #    failed_criteria.append("Status misalignment")
            
            print(f"\n‚ö†Ô∏è  AI COMPLIANCE ANALYSIS criteria not met - will enhance playbook")
            print(f"   Failed criteria: {', '.join(failed_criteria)}")
            #if not status_aligned:
            #    print(f"   Alignment details: {alignment_message}")
            #if has_issues and playbook_analysis_pass:
            #    print(f"   Note: PLAYBOOK ANALYSIS passed but has issues detected - enhancing to fix issues")
            #if not status_aligned and playbook_analysis_pass:
            #    print(f"   Note: PLAYBOOK ANALYSIS passed but status alignment failed - enhancing to fix alignment")
            # PLAYBOOK ANALYSIS: FAIL - playbook logic needs improvement
            state['error_message'] = analysis_message
            # Don't set should_retry here - let conditional edge decide
            
            if state['attempt'] < state['max_retries']:
                print(f"\n‚ö†Ô∏è  PLAYBOOK ANALYSIS: FAIL on attempt {state['attempt']}/{state['max_retries']}")
                print("üîÑ Enhancing playbook with analysis feedback...")
                
                # Prepare feedback message
                if extracted_advice:
                    # Use extracted advice if available
                    feedback_text = extracted_advice
                else:
                    # Extract the PLAYBOOK ANALYSIS section and recommendations
                    lines = analysis_message.split('\n')
                    feedback_lines = []
                    in_playbook_analysis = False
                    for i, line in enumerate(lines):
                        if "PLAYBOOK ANALYSIS" in line.upper():
                            in_playbook_analysis = True
                            feedback_lines.append(line)
                            # Get next few lines for context
                            for j in range(i+1, min(i+20, len(lines))):
                                if lines[j].strip().startswith('- **') and 'PLAYBOOK' not in lines[j].upper():
                                    break
                                feedback_lines.append(lines[j])
                            break
                    # Also look for RECOMMENDATION or PLAYBOOK LOGIC ISSUE sections
                    for i, line in enumerate(lines):
                        if any(kw in line.upper() for kw in ["PLAYBOOK LOGIC ISSUE", "RECOMMENDATION", "ADVICE"]):
                            if line not in feedback_lines:
                                feedback_lines.append(line)
                            # Get next 10-15 lines
                            for j in range(i+1, min(i+15, len(lines))):
                                if lines[j].strip().startswith('##') or (lines[j].strip().startswith('- **') and 'RECOMMENDATION' not in lines[j].upper()):
                                    break
                                if lines[j] not in feedback_lines:
                                    feedback_lines.append(lines[j])


                    if len(extracted_lines) > 0:
                        feedback_text = extracted_lines
                    else:
                        feedback_text = '\n'.join(feedback_lines).strip() or analysis_message[:1000]
                
                    ok = input('Continue langgraph_deepseek_generate_playbook.py? (enter at 404):')
                    print(f"feedback_lines: {feedback_lines}")
                    ok = input('Continue langgraph_deepseek_generate_playbook.py? (enter at 406):')
                    print(f"analysis_message[:1000]\n{analysis_message[:1000]}")
                    ok = input('Continue langgraph_deepseek_generate_playbook.py? (enter at 408):')

                feedback_header = "CRITICAL FIX REQUIRED: PLAYBOOK ANALYSIS: FAIL - The playbook has logic issues that need to be fixed."
                
                # Add analysis feedback to requirements
                analysis_escaped = feedback_text.replace('{', '{{').replace('}', '}}')
                state['requirements'].append(f"""{feedback_header}

Analysis Result:
{analysis_escaped}

INSTRUCTIONS TO FIX:
1. Review the PLAYBOOK ANALYSIS feedback carefully
2. Fix the playbook logic issues identified in the analysis
3. If analysis recommends conditional execution (e.g., "when: data_1 | length > 0"), implement it
4. If analysis identifies design flaws or logic issues, fix them according to the recommendations
5. Ensure the playbook follows CIS procedures correctly (e.g., skip subsequent requirements when first requirement returns nothing)
6. Update overall compliance logic to match CIS exactly
7. Make sure all requirements are executed in the correct order and conditions""")
        else:
            print("\n‚úÖ PLAYBOOK ANALYSIS: PASS - Playbook logic is correct!")
    
    return state


def execute_on_target_host_node(state: PlaybookGenerationState) -> PlaybookGenerationState:
    """LangGraph node: Execute playbook on target host."""
    print(f"\n{'='*80} execute_on_target_host_node")
    if state['test_host'] == state['target_host']:
        # Same host, already executed
        state['final_success'] = True
        state['final_output'] = state['test_output']
        state['workflow_complete'] = True
        
        # Display Analysis Result for same-host execution
        print("\n" + "=" * 80)
        print(f"üéä COMPLETE SUCCESS! Playbook executed on: {state['target_host']}!")
        print("=" * 80)
        
        # Perform full analysis (same as test host)
        print("\n" + "=" * 80)
        print(f"üìä Analysis Result for {state['target_host']}:")
        print("=" * 80)
        analysis_passed, analysis_message = analyze_playbook_output(
            requirements=state['requirements'],
            playbook_objective=state['playbook_objective'],
            test_output=state['test_output'],
            audit_procedure=state.get('audit_procedure'),
            playbook_content=state.get('playbook_content')  # Pass playbook content for analysis
        )
        print(analysis_message)
        print("=" * 80)
        
        return state
    
    print("\n" + "=" * 80)
    print(f"üöÄ FINAL EXECUTION: Running playbook on target host: {state['target_host']}")
    print("=" * 80)
    print(f"\nüìç Executing on: {state['target_host']}")
    print()
    
    final_success, final_output = test_playbook_on_server(
        state['filename'],
        state['target_host'],
        check_mode=False,
        verbose=True,  # Need verbose=True to capture compliance report output
        skip_debug=True  # Skip debug tasks on target host
    )
    
    state['final_success'] = final_success
    state['final_output'] = final_output
    state['workflow_complete'] = True
    
    if final_success:
        print("\n" + "=" * 80)
        print(f"üéä COMPLETE SUCCESS! Playbook executed on target: {state['target_host']}!")
        print("=" * 80)
        print("\n‚úÖ Final Execution Summary:")
        print("   1. ‚úÖ Syntax check passed")
        print(f"   2. ‚úÖ Test execution passed on {state['test_host']}")
        print(f"   3. ‚úÖ Final execution passed on {state['target_host']}")
        print("   4. ‚úÖ All requirements verified")
        
        print(f"\nüìã Full Final Execution Output from {state['target_host']}:")
        print("=" * 80)
        print(final_output)
        print("=" * 80)
        
        # Perform full analysis (same as test host)
        print("\n" + "=" * 80)
        print(f"üìä Analysis Result for {state['target_host']}:")
        print("=" * 80)
        analysis_passed, analysis_message = analyze_playbook_output(
            requirements=state['requirements'],
            playbook_objective=state['playbook_objective'],
            test_output=final_output,
            audit_procedure=state.get('audit_procedure')
        )
        print(analysis_message)
        print("=" * 80)
    else:
        print("\n" + "=" * 80)
        print(f"‚ö†Ô∏è  Execution on target host {state['target_host']} had issues")
        print("=" * 80)
        print(f"\nüìã Full Execution Output from {state['target_host']}:")
        print("=" * 80)
        print(final_output)
        print("=" * 80)
        
        # Perform full analysis even on failure
        print("\n" + "=" * 80)
        print(f"üìä Analysis Result for {state['target_host']}:")
        print("=" * 80)
        analysis_passed, analysis_message = analyze_playbook_output(
            requirements=state['requirements'],
            playbook_objective=state['playbook_objective'],
            test_output=final_output,
            audit_procedure=state.get('audit_procedure')
        )
        print(analysis_message)
        print("=" * 80)
    
    return state


def should_continue_after_syntax(state: PlaybookGenerationState) -> Literal["test_on_test_host", "retry", "end"]:
    """Conditional edge: Decide what to do after syntax check."""
    print(f"\n{'='*80} should_continue_after_syntax")
    if not state['syntax_valid']:
        if state['attempt'] < state['max_retries']:
            return "retry"
        else:
            return "end"
    return "test_on_test_host"


def should_continue_after_test(state: PlaybookGenerationState) -> Literal["analyze_output", "retry", "end"]:
    """Conditional edge: Decide what to do after test execution."""
    print(f"\n{'='*80} should_continue_after_test")
    # Check for connection errors - don't retry, just end
    if state.get('connection_error', False):
        return "end"  # Connection error - cannot validate, exit with warning
    
    if not state['test_success']:
        if state['attempt'] < state['max_retries']:
            return "retry"
        else:
            return "end"
    return "analyze_output"  # Test passed, now analyze output


def should_continue_after_analysis(state: PlaybookGenerationState) -> Literal["execute_on_target", "retry", "end"]:
    """Conditional edge: Decide what to do after output analysis."""
    print(f"\n{'='*80} should_continue_after_analysis")
    # Only regenerate if PLAYBOOK ANALYSIS: FAIL
    # The state['analysis_passed'] is already set based on PLAYBOOK ANALYSIS status
    if not state['analysis_passed']:
        if state['attempt'] < state['max_retries']:
            return "retry"
        else:
            return "end"
    return "execute_on_target"  # PLAYBOOK ANALYSIS: PASS, proceed to target


def should_continue_after_final(state: PlaybookGenerationState) -> Literal["end"]:
    """Conditional edge: After final execution, always end."""
    print(f"\n{'='*80} should_continue_after_final")
    return "end"


def create_playbook_workflow() -> StateGraph:
    """Create the LangGraph workflow for playbook generation."""
    
    # Create workflow graph
    workflow = StateGraph(PlaybookGenerationState)
    
    # Add nodes
    workflow.add_node("generate", generate_playbook_node)
    workflow.add_node("save", save_playbook_node)
    workflow.add_node("check_syntax", check_syntax_node)
    workflow.add_node("test_on_test_host", test_on_test_host_node)
    workflow.add_node("analyze_output", analyze_output_node)  # New analysis node
    workflow.add_node("execute_on_target", execute_on_target_host_node)
    workflow.add_node("increment_attempt", increment_attempt_node)  # Increment counter before retry
    
    # Define edges
    workflow.set_entry_point("generate")
    workflow.add_edge("generate", "save")
    workflow.add_edge("save", "check_syntax")
    workflow.add_edge("increment_attempt", "generate")  # After incrementing, regenerate
    
    # Conditional edges
    workflow.add_conditional_edges(
        "check_syntax",
        should_continue_after_syntax,
        {
            "test_on_test_host": "test_on_test_host",
            "retry": "increment_attempt",  # Go to increment node first
            "end": END
        }
    )
    
    workflow.add_conditional_edges(
        "test_on_test_host",
        should_continue_after_test,
        {
            "analyze_output": "analyze_output",  # Test passed -> Analyze
            "retry": "increment_attempt",  # Go to increment node first
            "end": END
        }
    )
    
    workflow.add_conditional_edges(
        "analyze_output",
        should_continue_after_analysis,
        {
            "execute_on_target": "execute_on_target",  # Analysis passed -> Execute
            "retry": "increment_attempt",  # Go to increment node first
            "end": END
        }
    )
    
    workflow.add_conditional_edges(
        "execute_on_target",
        should_continue_after_final,
        {
            "end": END
        }
    )
    
    return workflow.compile()


# Keep the same interface as the original script
#def generate_playbook(
#    playbook_objective: str,
#    target_host: str = "master-1",
#    become_user: str = "root",
#    requirements: list = None,
#    example_output: str = "",
#    audit_procedure: str = None
#):
#    """
#    Generate Ansible playbook based on custom requirements.
#    This function maintains the same interface as the original.
#    
#    Args:
#        playbook_objective: Description of what the playbook should achieve
#        target_host: Default target host for the playbook
#        become_user: User to become (usually root)
#        requirements: List of requirement strings describing what the playbook should do
#        example_output: Example command output to provide context
#        audit_procedure: CIS Benchmark audit procedure (script/commands)
#    
#    Returns:
#        str: Generated playbook content
#    """
#    # Use the original function for simple generation
#    return _original_generate_playbook(
#        playbook_objective=playbook_objective,
#        target_host=target_host,
#        become_user=become_user,
#        requirements=requirements,
#        example_output=example_output,
#        audit_procedure=audit_procedure
#    )


def generate_playbook_workflow(
    objective: str,
    requirements: list,
    target_host: str = "master-1",
    test_host: str = None,
    become_user: str = "root",
    filename: str = "generated_playbook.yml",
    example_output: str = "",
    audit_procedure: str = None,
    max_retries: int = None,
    verbose: bool = True
) -> dict:
    """
    Generate and execute an Ansible playbook using LangGraph workflow.
    
    This is the programmatic API that can be called from other Python scripts.
    
    Args:
        objective: Playbook objective description
        requirements: List of requirement strings
        target_host: Target host for execution
        test_host: Test host for validation (defaults to target_host)
        become_user: User to become when executing tasks
        filename: Output filename for generated playbook
        example_output: Example command output for context
        audit_procedure: CIS Benchmark audit procedure (optional)
        max_retries: Maximum retry attempts (auto-calculated if None)
        verbose: Print progress information
        
    Returns:
        dict: Final workflow state with results
        
    Raises:
        Exception: If workflow fails
    """
    import sys
    
    # Set defaults
    if test_host is None:
        test_host = target_host
    
    if max_retries is None:
        max_retries = max(len(requirements), 3)
        if verbose:
            print(f"\nüí° Auto-calculated max retries: {max_retries} (based on {len(requirements)} requirements)")
    
    # Display configuration
    if verbose:
        print("\n" + "=" * 80)
        print("üéØ CONFIGURATION (LangGraph Workflow)")
        print("=" * 80)
        print(f"Test Host:      {test_host}")
        if test_host != target_host:
            print(f"Target Host:    {target_host}")
        print(f"Become User:    {become_user}")
        print(f"Max Retries:    {max_retries}")
        print(f"Objective:      {objective[:60]}{'...' if len(objective) > 60 else ''}")
        print(f"Requirements:   {len(requirements)} items")
        if audit_procedure:
            print(f"Audit Proc:     {len(audit_procedure)} chars (CIS Benchmark audit procedure provided)")
        print(f"Filename:       {filename}")
        if test_host != target_host:
            print("\nüìã Execution Strategy:")
            print(f"   1. Test on: {test_host} (validation)")
            print(f"   2. Execute on: {target_host} (if test succeeds)")
        print("=" * 80)
    
    # Initialize state
    initial_state: PlaybookGenerationState = {
        "playbook_objective": objective,
        "target_host": target_host,
        "test_host": test_host,
        "become_user": become_user,
        "requirements": requirements.copy(),
        "example_output": example_output,
        "filename": filename,
        "max_retries": max_retries,
        "audit_procedure": audit_procedure or "",
        "attempt": 1,
        "playbook_content": "",
        "syntax_valid": False,
        "test_success": False,
        "analysis_passed": False,
        "analysis_message": "",
        "final_success": False,
        "error_message": "",
        "test_output": "",
        "final_output": "",
        "connection_error": False,
        "should_retry": False,
        "workflow_complete": False,
    }
    
    # Create and run workflow
    try:
        if verbose:
            print("\nüîÑ Starting LangGraph workflow...")
        workflow = create_playbook_workflow()
        
        # Execute workflow with increased recursion limit
        recursion_limit = max(50, max_retries * 3)
        final_state = workflow.invoke(
            initial_state,
            {"recursion_limit": recursion_limit}
        )
        
        # Check results
        # Handle connection errors separately
        if final_state.get('connection_error', False):
            if verbose:
                print("\n" + "="*80)
                print("‚ö†Ô∏è  WORKFLOW TERMINATED: Connection Error")
                print("="*80)
                print(f"   The playbook syntax is valid but cannot be validated on the host.")
                print(f"   Playbook file: {final_state['filename']}")
                print("="*80)
            # Return state but mark as incomplete due to connection error
            final_state['workflow_complete'] = False
            return final_state
        
        if final_state['workflow_complete'] and final_state['test_success']:
            if verbose:
                print("\n" + "="*80)
                print("üìä EXECUTION SUMMARY (LangGraph)")
                print("="*80)
                print(f"‚úÖ Workflow completed successfully!")
                print(f"   Total attempts: {final_state['attempt']}")
                print(f"   Playbook file: {final_state['filename']}")
                print("="*80)
            return final_state
        else:
            if verbose:
                print("\n" + "="*80)
                print("üìä EXECUTION SUMMARY (LangGraph)")
                print("="*80)
                print(f"‚ùå Workflow failed")
                print(f"   Total attempts: {final_state['attempt']}/{max_retries}")
                print(f"   Last error: {final_state['error_message'][:200]}")
                print("="*80)
            raise Exception(f"Workflow failed: {final_state['error_message']}")
            
    except Exception as e:
        if verbose:
            print(f"\n‚ùå Error in LangGraph workflow: {str(e)}")
            import traceback
            traceback.print_exc()
        raise


def main():
    """Main execution function using LangGraph workflow."""
    
    # Import argparse logic from original (keep it the same)
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Generate and execute Ansible playbooks using AI with LangGraph',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Use all defaults
  python3 langgraph_deepseek_generate_playbook.py
  
  # Specify custom target host
  python3 langgraph_deepseek_generate_playbook.py --target-host worker-1
  
  # Two-stage execution
  python3 langgraph_deepseek_generate_playbook.py \\
    --test-host 192.168.122.16 \\
    --target-host 192.168.122.17
"""
    )
    
    parser.add_argument('--target-host', '-t', type=str, default='master-1',
                        help='Target host to execute the playbook on')
    parser.add_argument('--test-host', type=str, default=None,
                        help='Test host for validation before target execution')
    parser.add_argument('--become-user', '-u', type=str, default='root',
                        help='User to become when executing tasks')
    parser.add_argument('--max-retries', '-r', type=int, default=None,
                        help='Maximum number of retry attempts')
    parser.add_argument('--objective', '-o', type=str,
                        default="Create an Ansible playbook that finds and kills processes that have 'packet_recvmsg' in their stack trace.",
                        help='Playbook objective')
    parser.add_argument('--requirement', action='append', dest='requirements',
                        help='Add a requirement (can be used multiple times)')
    parser.add_argument('--example-output', '-e', type=str,
                        default="""[root@master-1 ~]# egrep packet_recvmsg /proc/*/stack
/proc/2290657/stack:[<0>] packet_recvmsg+0x6e/0x4f0
grep: /proc/2290657/stack: No such file or directory

[root@master-1 ~]# ps -ef | grep -i 2290657
root     2290657 2526847  0 12:13 ?        00:00:00 ./bpfdoor
root     2822221 2819327  0 13:07 pts/1    00:00:00 grep --color=auto -i 2290657

[root@master-1 ~]# kill -9 2290657""",
                        help='Example command output')
    parser.add_argument('--filename', '-f', type=str,
                        default='kill_packet_recvmsg_process.yml',
                        help='Output filename for the generated playbook')
    parser.add_argument('--audit-procedure', type=str, default=None,
                        help='CIS Benchmark audit procedure (shell script or commands)')
    
    args = parser.parse_args()
    
    # Prepare parameters
    target_host = args.target_host
    test_host = args.test_host if args.test_host else target_host
    become_user = args.become_user
    playbook_objective = args.objective
    example_output = args.example_output
    filename = args.filename
    audit_procedure = args.audit_procedure
    
    if args.requirements:
        requirements = args.requirements
    else:
        requirements = [
            "Search for 'packet_recvmsg' keyword in /proc/*/stack",
            "Extract the process ID from the path",
            "Kill the identified process(es) using 'kill -9'",
            "Handle cases where: No matching processes found, Multiple processes found, Process disappears",
            "Display useful information: Show processes found, Show process details, Confirm termination"
        ]
    
    if args.max_retries is None:
        max_retries = max(len(requirements), 3)
        print(f"\nüí° Auto-calculated max retries: {max_retries} (based on {len(requirements)} requirements)")
    else:
        max_retries = args.max_retries
    
    # Display configuration
    print("\n" + "=" * 80)
    print("üéØ CONFIGURATION (LangGraph Workflow)")
    print("=" * 80)
    print(f"Test Host:      {test_host}")
    if test_host != target_host:
        print(f"Target Host:    {target_host}")
    print(f"Become User:    {become_user}")
    print(f"Max Retries:    {max_retries}")
    print(f"Objective:      {playbook_objective[:60]}{'...' if len(playbook_objective) > 60 else ''}")
    print(f"Requirements:   {len(requirements)} items")
    if audit_procedure:
        print(f"Audit Proc:     {len(audit_procedure)} chars (CIS Benchmark audit procedure provided)")
    print(f"Filename:       {filename}")
    if test_host != target_host:
        print("\nüìã Execution Strategy:")
        print(f"   1. Test on: {test_host} (validation)")
        print(f"   2. Execute on: {target_host} (if test succeeds)")
    print("=" * 80)
    
    # Initialize state
    initial_state: PlaybookGenerationState = {
        "playbook_objective": playbook_objective,
        "target_host": target_host,
        "test_host": test_host,
        "become_user": become_user,
        "requirements": requirements.copy(),
        "example_output": example_output,
        "filename": filename,
        "max_retries": max_retries,
        "audit_procedure": audit_procedure or "",  # CIS Benchmark audit procedure
        "attempt": 1,
        "playbook_content": "",
        "syntax_valid": False,
        "test_success": False,
        "analysis_passed": False,  # New field
        "analysis_message": "",     # New field
        "final_success": False,
        "error_message": "",
        "test_output": "",
        "final_output": "",
        "should_retry": False,
        "workflow_complete": False,
    }
    
    # Create and run workflow
    try:
        print("\nüîÑ Starting LangGraph workflow...")
        workflow = create_playbook_workflow()
        
        # Execute workflow with increased recursion limit
        # The limit should be at least 2x max_retries to account for all nodes per attempt
        recursion_limit = max(50, max_retries * 3)
        final_state = workflow.invoke(
            initial_state,
            {"recursion_limit": recursion_limit}
        )
        
        # Check results
        if final_state['workflow_complete'] and final_state['test_success']:
            print("\n" + "="*80)
            print("üìä EXECUTION SUMMARY (LangGraph)")
            print("="*80)
            print(f"‚úÖ Workflow completed successfully!")
            print(f"   Total attempts: {final_state['attempt']}")
            print(f"   Playbook file: {final_state['filename']}")
            print("="*80)
        else:
            print("\n" + "="*80)
            print("üìä EXECUTION SUMMARY (LangGraph)")
            print("="*80)
            print(f"‚ùå Workflow failed")
            print(f"   Total attempts: {final_state['attempt']}/{max_retries}")
            print(f"   Last error: {final_state['error_message'][:200]}")
            print("="*80)
            sys.exit(1)
            
    except Exception as e:
        print(f"\n‚ùå Error in LangGraph workflow: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
