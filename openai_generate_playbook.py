#!/usr/bin/env python3
"""
Generic Ansible Playbook Generator (OpenAI GPT-5-nano)

This script generates Ansible playbooks based on custom requirements using OpenAI's GPT-5-nano model.

HOW TO USE FOR DIFFERENT TASKS:
================================

In the main() function, modify these variables:

1. playbook_objective: 
   What the playbook should achieve (clear description)

2. target_host: 
   Default target host for the playbook

3. become_user: 
   User to become (usually "root")

4. requirements: 
   List of specific requirements as strings
   - What tasks to perform
   - How to handle errors
   - What output to display

5. example_output: 
   Example command output to provide context


EXAMPLE - Disk Cleanup Playbook:
=================================

playbook_objective = "Clean up old log files to free disk space"
target_host = "webserver-1"
become_user = "root"
requirements = [
    "Find log files older than 30 days in /var/log",
    "Display files to be deleted",
    "Delete old log files",
    "Show disk space freed"
]
example_output = "df -h output and find command results"

"""

import os
import sys
import subprocess
import argparse
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# Load environment variables
load_dotenv()

# Initialize OpenAI model
# Get API key from environment variable
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

if not OPENAI_API_KEY:
    print("‚ö†Ô∏è  Warning: OPENAI_API_KEY not set in environment")
    print("   Please set it in your .env file or export it:")
    print("   export OPENAI_API_KEY='your_api_key_here'")
    print("   Or add to .env file:")
    print("   OPENAI_API_KEY=your_api_key_here")

model = ChatOpenAI(
    model="gpt-5-nano",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=3,
    api_key=OPENAI_API_KEY
)
def generate_playbook(
    playbook_objective: str,
    target_host: str = "master-1",
    become_user: str = "root",
    requirements: list = None,
    example_output: str = ""
):
    """
    Generate Ansible playbook based on custom requirements.
    
    Args:
        playbook_objective: Description of what the playbook should achieve
        target_host: Default target host for the playbook
        become_user: User to become (usually root)
        requirements: List of requirement strings describing what the playbook should do
        example_output: Example command output to provide context
    """
    
    if requirements is None:
        requirements = []
    
    # Format requirements list for the prompt
    requirements_text = "\n".join([f"{i+1}. {req}" for i, req in enumerate(requirements)])
    
    # Build example output section if provided
    example_section = ""
    if example_output:
        example_section = f"""

**Example Output Context:**
```bash
{example_output}
```"""
    
    prompt_template = f"""You are an expert Ansible playbook developer. Generate a complete, production-ready Ansible playbook based on the following requirements:

**Objective:**
{playbook_objective}

**Requirements:**
{requirements_text}{example_section}

**CRITICAL REQUIREMENTS FOR VERIFICATION PLAYBOOKS:**
- This is a VERIFICATION/COMPLIANCE CHECK playbook - it must NEVER fail
- Use ignore_errors: true for ALL verification tasks
- Store results of each check in variables (use register)
- Each verification task should set a compliance status variable (compliant_X: true/false)
- Create a final summary task that reports:
  * Which requirements are COMPLIANT (‚úÖ)
  * Which requirements are NON-COMPLIANT (‚ùå)
  * Overall compliance status (COMPLIANT or NON-COMPLIANT)
- The playbook should ALWAYS complete successfully regardless of findings

**CRITICAL: FORMAT THE COMPLIANCE REPORT CORRECTLY:**
The final compliance report MUST use a LIST of strings in the debug msg, NOT a single string with \\n.

CORRECT FORMAT (use this):
```yaml
- name: Generate compliance report
  debug:
    msg:
      - "============================================"
      - "    COMPLIANCE VERIFICATION REPORT"
      - "============================================"
      - ""
      - "CHECK RESULTS:"
      - "  1. OS Version: {{{{ '‚úÖ COMPLIANT' if compliant_os else '‚ùå NON-COMPLIANT' }}}}"
      - "  2. Package X: {{{{ '‚úÖ COMPLIANT' if compliant_pkg else '‚ùå NON-COMPLIANT' }}}}"
      - ""
      - "============================================"
      - "OVERALL STATUS: {{{{ 'COMPLIANT ‚úÖ' if overall_compliant else 'NON-COMPLIANT ‚ùå' }}}}"
      - "============================================"
```

WRONG FORMAT (do NOT use):
```yaml
- name: Generate compliance report
  debug:
    msg: "Line1\\nLine2\\nLine3"  # This displays \\n literally!
```

**Technical Details:**
- **CRITICAL: ALWAYS gather facts FIRST** (gather_facts: yes at playbook level)
- **NEVER use variables before checking if they are defined**
- Use 'when: variable_name is defined' before accessing any variable
- Prefer ansible_facts dict over ansible_* variables (e.g., ansible_facts['distribution'])
- **CRITICAL: To check if a package is INSTALLED, use package_facts module**
  * Do NOT use `ansible.builtin.package` with `state: present` and `check_mode: yes`
  * That only checks if package CAN be installed, not if it IS installed
  * Use `ansible.builtin.package_facts` to get actual installed packages
- **IMPORTANT: Match compliance logic to requirement intent**
  * "SHOULD be present" ‚Üí COMPLIANT if installed, NON-COMPLIANT if not
  * "SHOULD NOT be present" ‚Üí COMPLIANT if not installed, NON-COMPLIANT if installed
  * "SHOULD be running" ‚Üí COMPLIANT if running, NON-COMPLIANT if not
  * Always verify the logic matches the requirement direction
- **IMPORTANT: Use Ansible modules FIRST** before shell/command
  * Use ansible.builtin.package or ansible.builtin.yum/dnf for package operations
  * Use ansible.builtin.service or ansible.builtin.systemd for service management
  * Use ansible.builtin.stat for file/directory checks
  * Use ansible.builtin.lineinfile or ansible.builtin.copy for file operations
  * Use ansible.builtin.command_line for command output parsing
  * Use ansible.builtin.setup or gather_facts for system information
  * Only use ansible.builtin.shell or ansible.builtin.command when:
    - No suitable module exists for the task
    - Need to use pipes, redirects, or shell features
    - Parsing complex command output with grep/awk/sed
- Use register to capture ALL outputs (from both modules and commands)
- **CRITICAL: Add debug task after EVERY register to show what was captured**
  * This helps verify the playbook meets requirements
  * Shows intermediate values for troubleshooting
  * Makes the playbook logic transparent and verifiable
- Store compliance results: set_fact to track compliant_requirement_X: true/false
- Use ignore_errors: true on ALL verification tasks
- Use failed_when: false to prevent task failures
- Add debug messages showing check results
- Create a final COMPLIANCE REPORT section
- Use changed_when: false for verification tasks (they don't change system state)

**CRITICAL: AVOID UNDEFINED VARIABLE BUGS:**
The most common playbook bug is using undefined variables. Follow these rules:

1. ALWAYS gather facts at playbook start:
```yaml
- name: Verify Environment
  hosts: all
  become: yes
  gather_facts: yes  # MANDATORY - sets ansible_facts
```

2. Check if variables exist before using them:
```yaml
# BAD: Will fail if variable doesn't exist
- name: Use minor version
  set_fact:
    min_version: "{{{{ ansible_distribution_minor_version }}}}"

# GOOD: Check first
- name: Use minor version
  set_fact:
    min_version: "{{{{ ansible_distribution_minor_version }}}}"
  when: ansible_distribution_minor_version is defined
```

3. Use ansible_facts dict (safer, always available after gather_facts):
```yaml
# BETTER: Use ansible_facts which are guaranteed after gather_facts
- name: Check OS version
  debug:
    msg: "OS: {{{{ ansible_facts['distribution'] }}}} {{{{ ansible_facts['distribution_version'] }}}}"
```

**CRITICAL: PROPER JINJA2 SYNTAX:**
Always close Jinja2 control structures properly:

1. Every {{{{% if %}}}} must have a matching {{{{% endif %}}}}:
```yaml
# BAD: Missing endif
- name: Set status
  set_fact:
    status: "{{{{% if compliant %}}}}PASS"  # ERROR: No endif!

# GOOD: Properly closed
- name: Set status
  set_fact:
    status: "{{{{% if compliant %}}}}PASS{{{{% else %}}}}FAIL{{{{% endif %}}}}"
```

2. Use ternary operator for simple conditions:
```yaml
# BEST: Use ternary operator (no if/endif needed)
- name: Set status
  set_fact:
    status: "{{{{ 'PASS' if compliant else 'FAIL' }}}}"
```

3. Multi-line Jinja2 in debug messages:
```yaml
# BAD: Unclosed if statement
- debug:
    msg: "{{{{% if not compliant %}}}}FAILED"  # ERROR!

# GOOD: Use ternary in each line
- debug:
    msg:
      - "Status: {{{{ 'PASS' if compliant else 'FAIL' }}}}"
      - "Details: {{{{ details | default('N/A') }}}}"
```

**Module Priority Examples:**
```yaml
# GOOD: Use package_facts to check if package is installed
- name: Gather package facts
  ansible.builtin.package_facts:
    manager: auto
  register: pkg_facts
  ignore_errors: true
  failed_when: false

- name: Check if package is installed
  set_fact:
    pkg_installed: "{{{{ 'docker' in (pkg_facts.ansible_facts.packages | default({{{{}}}})) }}}}"

- name: Debug package check 
  debug:
    var: pkg_facts
  tags: [debug]

- name: Show package status
  debug:
    msg: "Docker package: {{{{ 'Installed' if pkg_installed | default(false) else 'Not installed' }}}}"
  tags: [debug]

# WRONG: Using package module with state:present in check_mode for verification
# This will succeed even if package is NOT installed!
- name: WRONG - Do not use this pattern for verification
  ansible.builtin.package:
    name: docker
    state: present
  register: pkg_check
  check_mode: yes
  # Problem: check_mode with state:present succeeds even if package not installed
  # It only checks if package CAN be installed, not if it IS installed

# GOOD: Use service module for service status
- name: Check service status
  ansible.builtin.service:
    name: docker
    state: started
  register: service_status
  check_mode: yes
  ignore_errors: true
  failed_when: false

- name: Debug service status result
  debug:
    var: service_status
  tags: [debug]

- name: Show service status
  debug:
    msg: "Service status: {{{{ service_status.status.ActiveState | default('unknown') }}}}"
  tags: [debug]

# GOOD: Use stat module for file/directory existence
- name: Check if file exists
  ansible.builtin.stat:
    path: /etc/systemd/system.conf
  register: config_file
  ignore_errors: true
  failed_when: false

- name: Debug file stat result
  debug:
    var: config_file
  tags: [debug]

- name: Show file existence
  debug:
    msg: "Config file exists: {{{{ config_file.stat.exists | default(false) }}}}"
  tags: [debug]

# ACCEPTABLE: Use command when no module exists, with register and debug
- name: Check for specific log entries
  ansible.builtin.command: journalctl -n 100 --no-pager
  register: journal_check
  ignore_errors: true
  failed_when: false
  changed_when: false

- name: Debug journal output (first 500 chars)
  debug:
    msg: "{{{{ journal_check.stdout[:500] if journal_check.stdout is defined else 'No output' }}}}"
  tags: [debug]

- name: Analyze journal for errors
  set_fact:
    journal_has_errors: "{{{{ 'error' in (journal_check.stdout | lower) }}}}"

- name: Show journal analysis
  debug:
    msg: "Journal has errors: {{{{ journal_has_errors }}}}"
  tags: [debug]

# LAST RESORT: Use shell only when pipes/redirects needed, with register and debug
- name: Check complex pattern in logs
  ansible.builtin.shell: journalctl -n 100 | grep -c "error pattern"
  register: error_count
  ignore_errors: true
  failed_when: false
  changed_when: false

- name: Debug error count result
  debug:
    var: error_count
  tags: [debug]

- name: Show error count
  debug:
    msg: "Errors found: {{{{ error_count.stdout | default('0') }}}}"
  tags: [debug]
```

**Playbook Structure:**
CRITICAL: Use "hosts: all" in the playbook, NOT a specific hostname.

1. Gather Facts
2. Initialize compliance tracking variables
3. Run verification tasks with this pattern for EACH requirement:
   a. Execute the check task (with register)
   b. Add debug task to show what was captured 
   c. Analyze the result and set compliance variable
   d. Add debug task to show compliance status
4. Generate final compliance report showing:
   - List of compliant requirements
   - List of non-compliant requirements  
   - Overall status

**DEBUG OUTPUT GUIDELINES:**
For each registered variable, add TWO debug tasks:
1. Full variable dump (debug) - Shows complete structure for troubleshooting
2. Human-readable summary - Shows the key information extracted

**CRITICAL: Tag all troubleshooting debug tasks with 'tags: [debug]'**
This allows them to be skipped in production execution while keeping them available for testing.

Example pattern:
```yaml
# Step 1: Execute check
- name: Check something
  some_module:
    param: value
  register: check_result
  ignore_errors: true
  failed_when: false

# Step 2: Debug full result (TAGGED - will be skipped on target host)
- name: Debug full check result
  debug:
    var: check_result
  tags: [debug]

# Step 3: Extract and show key information (TAGGED - will be skipped on target host)
- name: Show check status
  debug:
    msg: "Check result: {{{{ 'Success' if check_result is succeeded else 'Failed' }}}}"
  tags: [debug]

# Step 4: Set compliance variable (NOT tagged - always runs)
- name: Record compliance status
  set_fact:
    compliant_something: "{{{{ check_result is succeeded }}}}"
```

**Debug Task Tagging Rules:**
- Tag ALL debug tasks showing intermediate results with: tags: [debug]
- Tag ALL debug tasks with: tags: [debug]
- Tag debug tasks showing extracted values (like "Package: Installed") with: tags: [debug]
- DO NOT tag the final compliance report (users always need to see it)
- DO NOT tag set_fact tasks (they're needed for logic)
- Only the final summary/compliance report should be untagged

**Output Format:**
- Provide ONLY valid YAML playbook content
- Start with --- (YAML document marker)
- Do NOT include markdown code blocks or language identifiers
- Include helpful comments
- The playbook must COMPLETE SUCCESSFULLY even if checks fail
- Include a final summary/report task
- MUST use "hosts: all" not a specific hostname

Example playbook structure:
```yaml
---
- name: Playbook Name
  hosts: all
  become: yes
  gather_facts: yes
  
  tasks:
    # ... tasks here ...
```

Generate the complete Ansible playbook now:"""

    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | model
    
    print("Generating Ansible playbook...")
    print("=" * 80)
    
    response = chain.invoke({})
    playbook_content = response.content
    
    # Clean up the response - remove markdown code blocks if present
    if "```yaml" in playbook_content:
        playbook_content = playbook_content.split("```yaml")[1].split("```")[0].strip()
    elif "```" in playbook_content:
        playbook_content = playbook_content.split("```")[1].split("```")[0].strip()
    
    # Remove any stray "yaml" or "yml" line at the beginning
    lines = playbook_content.split('\n')
    while lines and lines[0].strip().lower() in ['yaml', 'yml', '']:
        lines.pop(0)
    playbook_content = '\n'.join(lines)
    
    # Ensure it starts with ---
    if not playbook_content.strip().startswith('---'):
        playbook_content = '---\n' + playbook_content
    
    return playbook_content


def save_playbook(content: str, filename: str = "kill_packet_recvmsg_process.yml"):
    """Save the generated playbook to a file."""
    with open(filename, 'w') as f:
        f.write(content)
    print(f"\n‚úÖ Playbook saved to: {filename}")


def check_playbook_syntax(filename: str, target_host: str) -> tuple[bool, str]:
    """
    Check Ansible playbook syntax.
    
    Args:
        filename: Path to the playbook file
        
    Returns:
        tuple: (is_valid, error_message)
    """
    try:
        print(f"\nüîç Checking playbook syntax: {filename}")
        cmd = [
            'ansible-navigator', 'run', 
            filename, 
            '-i', f'{target_host},',
            '-u', 'root',  # Use root user to connect
            '-v',  # Verbose output
            '--syntax-check',
            '--mode', 'stdout'  # Force output to stdout instead of interactive mode
        ]
        print(f"Command: {' '.join(cmd)}")
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            print("‚úÖ Syntax check passed!")
            return True, ""
        else:
            # Combine stdout and stderr
            error_output = []
            
            if result.stdout and result.stdout.strip():
                error_output.append("=== STDOUT ===")
                error_output.append(result.stdout)
            
            if result.stderr and result.stderr.strip():
                error_output.append("=== STDERR ===")
                error_output.append(result.stderr)
            
            # If both are empty, provide helpful message
            if not error_output:
                error_output.append("No error output captured. Checking playbook file...")
                # Try to validate the YAML directly
                try:
                    import yaml
                    with open(filename, 'r') as f:
                        yaml.safe_load_all(f)
                    error_output.append("YAML structure is valid. Issue may be with Ansible-specific syntax.")
                except yaml.YAMLError as e:
                    error_output.append(f"YAML Parsing Error: {str(e)}")
                except Exception as e:
                    error_output.append(f"Error reading file: {str(e)}")
            
            error_msg = "\n".join(error_output)
            
            print(f"‚ùå Syntax check failed!")
            print("\n" + "="*80)
            print("SYNTAX ERROR DETAILS:")
            print("="*80)
            print(error_msg)
            print("="*80)
            
            return False, error_msg
            
    except subprocess.TimeoutExpired:
        error_msg = "Syntax check timed out after 30 seconds"
        print(f"‚ùå {error_msg}")
        return False, error_msg
    except FileNotFoundError:
        error_msg = "ansible-navigator command not found. Please ensure Ansible is installed."
        print(f"‚ùå {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = f"Error during syntax check: {str(e)}"
        print(f"‚ùå {error_msg}")
        return False, error_msg


def test_playbook_on_server(filename: str, target_host: str = "192.168.122.16", check_mode: bool = False, verbose: bool = True, skip_debug: bool = False) -> tuple[bool, str]:
    """
    Test the playbook on a real server to verify it meets requirements.
    
    Args:
        filename: Path to the playbook file
        target_host: Target server IP/hostname
        check_mode: If True, run in check mode (dry-run, no changes made)
        verbose: If True, add -v flag for verbose output
        skip_debug: If True, skip debug-tagged tasks (for production execution)
        
    Returns:
        tuple: (is_successful, output)
    """
    try:
        mode_desc = "check mode (dry-run)" if check_mode else "execution mode"
        if skip_debug:
            mode_desc += " [skipping debug tasks]"
        print(f"\nüß™ Testing playbook on server: {target_host} ({mode_desc})")
        
        # Build ansible-navigator command
        cmd = [
            'ansible-navigator', 'run', 
            filename, 
            '-i', f'{target_host},',
            '-u', 'root'  # Use root user to connect
        ]
        
        if verbose:
            cmd.append('-v')

        if check_mode:
            cmd.append('--check')  # Dry-run mode
        
        if skip_debug:
            cmd.extend(['--skip-tags', 'debug'])  # Skip troubleshooting debug tasks
            
        
        print(f"   Running: {' '.join(cmd)}")
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120  # 2 minutes timeout for execution
        )
        
        output = result.stdout + result.stderr
        
        # Check for PLAYBOOK BUGS that require retry/regeneration
        playbook_bug_patterns = [
            ("undefined variable", "Undefined variable error"),
            ("is undefined", "Variable is undefined"),
            ("'dict object' has no attribute", "Invalid attribute access"),
            ("Syntax Error while loading YAML", "YAML syntax error"),
            ("template error while templating string", "Jinja2 template error"),
            ("Unexpected end of template", "Jinja2 unclosed block"),
            ("expected token 'end of print statement'", "Jinja2 syntax error"),
            ("Jinja was looking for the following tags", "Jinja2 missing closing tag"),
            ("The error was:", "Ansible task error"),
            ("undefined method", "Undefined method call"),
            ("cannot be converted to", "Type conversion error"),
            ("Invalid/incorrect password", "Authentication error in task"),
        ]
        
        for pattern, description in playbook_bug_patterns:
            if pattern in output:
                # Check if it's being ignored (has "...ignoring" after the error)
                # Even if ignored, undefined variables are still bugs
                
                print(f"‚ùå PLAYBOOK BUG DETECTED: {description}")
                print("   This is a playbook error, not a verification failure")
                print("   The playbook needs to be regenerated with corrections")
                print("\n" + "="*80)
                print("ERROR DETAILS:")
                print("="*80)
                # Extract and show the error context
                error_lines = output.split('\n')
                error_context_lines = []
                for i, line in enumerate(error_lines):
                    if pattern in line:
                        # Show surrounding context
                        start = max(0, i-3)
                        end = min(len(error_lines), i+8)
                        error_context_lines = error_lines[start:end]
                        print('\n'.join(error_context_lines))
                        break
                print("="*80)
                
                # Return detailed error with full context
                full_error_context = '\n'.join(error_context_lines) if error_context_lines else output[:500]
                return False, f"PLAYBOOK BUG: {description}\n\nError context:\n{full_error_context}\n\nFull pattern: {pattern}"

        if result.returncode == 0:
            print(f"‚úÖ Playbook executed successfully in {mode_desc}!")
            
            # For verification/compliance playbooks, success means it completed
            # We don't require failed=0 because checks are allowed to find non-compliance
            if "ok=" in output and "PLAY RECAP" in output:
                # Check PLAY RECAP for actual task failures
                # Look for "failed=N" where N > 0
                import re
                recap_match = re.search(r'failed=(\d+)', output)
                if recap_match:
                    failed_count = int(recap_match.group(1))
                    if failed_count > 0:
                        print(f"‚ö†Ô∏è  Playbook has {failed_count} failed task(s)")
                        # This could be a playbook bug, return failure to trigger retry
                        return False, f"Playbook had {failed_count} failed tasks\n\n{output}"
                
                print("‚úÖ Playbook completed successfully!")
                
                # Check for compliance report
                if "COMPLIANT" in output or "NON-COMPLIANT" in output or "Compliance" in output:
                    print("‚úÖ Compliance report generated")
                
                # Parse Ansible output for specific checks
                if "PLAY RECAP" in output:
                    recap_start = output.find("PLAY RECAP")
                    recap_section = output[recap_start:recap_start+200].split("\n")[0:4]
                    for line in recap_section:
                        if line.strip():
                            print(f"   {line}")
                
                return True, output
            else:
                return False, f"Execution completed but output format unexpected:\n{output}"
        else:
            print(f"‚ö†Ô∏è  Playbook execution returned code: {result.returncode}")
            
            # Check if it's an SSH/connection issue
            if "Failed to connect to the host" in output or "Permission denied" in output:
                print("‚ö†Ô∏è  SSH connection issue detected")
                print("   Falling back to check mode only (syntax validation)")
                return True, "SSH unavailable - syntax validated only"
            
            ## Check for PLAYBOOK BUGS that require retry/regeneration
            #playbook_bug_patterns = [
            #    ("undefined variable", "Undefined variable error"),
            #    ("is undefined", "Variable is undefined"),
            #    ("'dict object' has no attribute", "Invalid attribute access"),
            #    ("Syntax Error while loading YAML", "YAML syntax error"),
            #    ("The error was:", "Ansible task error"),
            #    ("undefined method", "Undefined method call"),
            #    ("cannot be converted to", "Type conversion error"),
            #    ("Invalid/incorrect password", "Authentication error in task"),
            #]
            #
            #for pattern, description in playbook_bug_patterns:
            #    if pattern in output:
            #        # Check if it's being ignored (has "...ignoring" after the error)
            #        # Even if ignored, undefined variables are still bugs
            #        if "undefined" in pattern.lower():
            #            print(f"‚ùå PLAYBOOK BUG DETECTED: {description}")
            #            print("   This is a playbook error, not a verification failure")
            #            print("   The playbook needs to be regenerated with corrections")
            #            print("\n" + "="*80)
            #            print("ERROR DETAILS:")
            #            print("="*80)
            #            # Extract and show the error context
            #            error_lines = output.split('\n')
            #            error_context_lines = []
            #            for i, line in enumerate(error_lines):
            #                if pattern in line:
            #                    # Show surrounding context
            #                    start = max(0, i-3)
            #                    end = min(len(error_lines), i+8)
            #                    error_context_lines = error_lines[start:end]
            #                    print('\n'.join(error_context_lines))
            #                    break
            #            print("="*80)
            #            
            #            # Return detailed error with full context
            #            full_error_context = '\n'.join(error_context_lines) if error_context_lines else output[:500]
            #            return False, f"PLAYBOOK BUG: {description}\n\nError context:\n{full_error_context}\n\nFull pattern: {pattern}"
            
            # Check if it's an OS version mismatch (playbook is valid, just wrong target)
            os_version_patterns = [
                "This playbook only supports Red Hat Enterprise Linux",
                "ansible_distribution_major_version",
                "OS version mismatch",
                "distribution version",
                "Only supported on"
            ]
            
            if any(pattern in output for pattern in os_version_patterns):
                print("‚ö†Ô∏è  OS version mismatch detected")
                print("   The playbook is valid but targets a different OS version than the test host")
                print("   This is expected when KCS article specifies a different OS version")
                print("   ‚úÖ Treating as successful generation - playbook syntax and logic are correct")
                return True, "OS version mismatch - playbook valid for different OS version"
            
            # For verification playbooks, even non-zero exit codes might be acceptable
            # if the playbook completed and generated a report
            if "PLAY RECAP" in output and ("COMPLIANT" in output or "Compliance" in output):
                print("‚ö†Ô∏è  Playbook exited with non-zero code but completed verification")
                print("   This is acceptable for compliance check playbooks")
                print("   ‚úÖ Treating as successful - compliance report was generated")
                return True, "Compliance verification completed with findings"
            
            return False, output
            
    except subprocess.TimeoutExpired:
        error_msg = "Playbook execution timed out after 120 seconds"
        print(f"‚ùå {error_msg}")
        return False, error_msg
    except FileNotFoundError:
        error_msg = "ansible-navigator command not found. Please ensure Ansible is installed."
        print(f"‚ùå {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = f"Error during playbook testing: {str(e)}"
        print(f"‚ùå {error_msg}")
        return False, error_msg


def main():
    """Main execution function."""
    
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description='Generate and execute Ansible playbooks using AI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Use all defaults (packet_recvmsg killer)
  python3 deepseek_generate_playbook.py
  
  # Specify custom target host
  python3 deepseek_generate_playbook.py --target-host worker-1
  
  # Custom objective and filename
  python3 deepseek_generate_playbook.py \\
    --objective "Clean up old log files" \\
    --filename cleanup_logs.yml
  
  # Add custom requirements
  python3 deepseek_generate_playbook.py \\
    --objective "Install Docker" \\
    --requirement "Install Docker packages" \\
    --requirement "Start Docker service" \\
    --requirement "Enable Docker at boot" \\
    --filename install_docker.yml
  
  # Full customization
  python3 deepseek_generate_playbook.py \\
    --target-host worker-1 \\
    --become-user ansible \\
    --objective "Update system packages" \\
    --requirement "Update package cache" \\
    --requirement "Upgrade all packages" \\
    --filename system_update.yml
"""
    )
    
    parser.add_argument(
        '--target-host', '-t',
        type=str,
        default='master-1',
        help='Target host to execute the playbook on (default: master-1)'
    )
    
    parser.add_argument(
        '--test-host',
        type=str,
        default=None,
        help='Test host for validation before target execution (if not specified, uses target-host for testing)'
    )
    
    parser.add_argument(
        '--become-user', '-u',
        type=str,
        default='root',
        help='User to become when executing tasks (default: root)'
    )
    
    
    parser.add_argument(
        '--max-retries', '-r',
        type=int,
        default=None,
        help='Maximum number of retry attempts (default: auto-calculated based on number of requirements)'
    )
    
    parser.add_argument(
        '--objective', '-o',
        type=str,
        default="Create an Ansible playbook that finds and kills processes that have 'packet_recvmsg' in their stack trace.",
        help='Playbook objective - what the playbook should achieve'
    )
    
    parser.add_argument(
        '--requirement',
        action='append',
        dest='requirements',
        help='Add a requirement (can be used multiple times). If not specified, uses default packet_recvmsg requirements.'
    )
    
    parser.add_argument(
        '--example-output', '-e',
        type=str,
        default="""[root@master-1 ~]# egrep packet_recvmsg /proc/*/stack
/proc/2290657/stack:[<0>] packet_recvmsg+0x6e/0x4f0
grep: /proc/2290657/stack: No such file or directory

[root@master-1 ~]# ps -ef | grep -i 2290657
root     2290657 2526847  0 12:13 ?        00:00:00 ./bpfdoor
root     2822221 2819327  0 13:07 pts/1    00:00:00 grep --color=auto -i 2290657

[root@master-1 ~]# kill -9 2290657""",
        help='Example command output to provide context to the LLM'
    )
    
    parser.add_argument(
        '--filename', '-f',
        type=str,
        default='kill_packet_recvmsg_process.yml',
        help='Output filename for the generated playbook (default: kill_packet_recvmsg_process.yml)'
    )
    
    args = parser.parse_args()
    
    # Use command-line arguments
    target_host = args.target_host
    test_host = args.test_host if args.test_host else target_host
    become_user = args.become_user
    playbook_objective = args.objective
    example_output = args.example_output
    filename = args.filename
    
    # Handle requirements - use provided ones or defaults
    if args.requirements:
        requirements = args.requirements
    else:
        # Default requirements for packet_recvmsg killer
        requirements = [
            "Search for 'packet_recvmsg' keyword in /proc/*/stack",
            "Extract the process ID from the path (e.g., from '/proc/2290657/stack', extract '2290657')",
            "Kill the identified process(es) using 'kill -9'",
            "Handle cases where: No matching processes are found, Multiple processes are found (kill all), Process disappears between detection and kill attempt",
            "Display useful information: Show what processes were found, Show the process details (ps output), Confirm successful termination"
        ]
    
    # Calculate max_retries based on number of requirements if not specified
    if args.max_retries is None:
        max_retries = max(len(requirements), 3)  # At least 3, or number of requirements
        print(f"\nüí° Auto-calculated max retries: {max_retries} (based on {len(requirements)} requirements)")
    else:
        max_retries = args.max_retries
    
    # Display configuration
    print("\n" + "=" * 80)
    print("üéØ CONFIGURATION")
    print("=" * 80)
    print(f"Test Host:      {test_host}")
    if test_host != target_host:
        print(f"Target Host:    {target_host}")
    print(f"Become User:    {become_user}")
    print(f"Max Retries:    {max_retries}")
    print(f"Objective:      {playbook_objective[:60]}{'...' if len(playbook_objective) > 60 else ''}")
    print(f"Requirements:   {len(requirements)} items")
    print(f"Filename:       {filename}")
    if test_host != target_host:
        print("\nüìã Execution Strategy:")
        print(f"   1. Test on: {test_host} (validation)")
        print(f"   2. Execute on: {target_host} (if test succeeds)")
    print("=" * 80)
    
    try:
        for attempt in range(1, max_retries + 1):
            print(f"\n{'='*80}")
            print(f"Attempt {attempt}/{max_retries}: Generating Ansible playbook...")
            print("=" * 80)
            print(f"Objective: {playbook_objective}")
            print(f"Test Host: {test_host}")
            if test_host != target_host:
                print(f"Target Host: {target_host}")
            print(f"Become User: {become_user}")
            print(f"Requirements: {len(requirements)} items")
            print("=" * 80)
            
            # Generate the playbook
            playbook = generate_playbook(
                playbook_objective=playbook_objective,
                target_host=test_host,  # Use test_host for generation
                become_user=become_user,
                requirements=requirements,
                example_output=example_output
            )
            
            # Display the generated playbook
            print("\nüìã Generated Ansible Playbook:")
            print("=" * 80)
            print(playbook)
            print("=" * 80)
            
            # Save to file
            save_playbook(playbook, filename)
            
            # Check syntax
            is_valid, error_msg = check_playbook_syntax(filename, test_host)
            
            if not is_valid:
                # Syntax check failed
                if attempt < max_retries:
                    print(f"\n‚ö†Ô∏è  Syntax check failed on attempt {attempt}/{max_retries}")
                    print("üîÑ Retrying with additional instructions to LLM...")
                    print("\nüìã Error Summary:")
                    # Extract key error information
                    error_lines = error_msg.split('\n')
                    for line in error_lines[:10]:  # Show first 10 lines of error
                        if line.strip():
                            print(f"   {line}")
                    if len(error_lines) > 10:
                        print(f"   ... ({len(error_lines) - 10} more lines)")
                    
                    # Escape curly braces in error message to prevent format string errors
                    error_msg_escaped = error_msg[:200].replace('{', '{{').replace('}', '}}')
                    
                    # Add error context to requirements for next attempt
                    requirements.append(f"IMPORTANT: Previous attempt had syntax error: {error_msg_escaped}")
                    continue
                else:
                    print(f"\n‚ùå Failed to generate valid playbook after {max_retries} attempts")
                    print("\n" + "="*80)
                    print("FINAL SYNTAX ERROR:")
                    print("="*80)
                    print(error_msg)
                    print("="*80)
                    print(f"\n‚ö†Ô∏è  The playbook has been saved to: {filename}")
                    print("Please review and fix the syntax errors manually.")
                    raise Exception(f"Syntax validation failed after {max_retries} attempts")
            
            # Syntax is valid, now test on test host
            print("\n" + "=" * 80)
            print(f"‚úÖ Syntax Valid! Now testing on test host: {test_host}...")
            print("=" * 80)
            
            # Execute on test host first
            test_success, test_output = test_playbook_on_server(filename, test_host, check_mode=False, verbose=True, skip_debug=False)
            
            if test_success:
                # Test on test_host succeeded
                print("\n" + "=" * 80)
                print(f"üéâ SUCCESS! Playbook validated on test host: {test_host}!")
                print("=" * 80)
                print("\n‚úÖ Test Execution Summary:")
                print("   1. ‚úÖ Syntax check passed")
                print(f"   2. ‚úÖ Test execution passed on {test_host}")
                print("   3. ‚úÖ All requirements verified")
                
                # Check if it was an OS version mismatch
                if "OS version mismatch" in test_output or "playbook valid for different OS version" in test_output:
                    print("\n‚ö†Ô∏è  NOTE: OS Version Mismatch on Test Host")
                    print("   The playbook is designed for a different OS version than the test host.")
                    print("   The playbook itself is syntactically and logically correct.")
                
                # Show FULL test execution output
                print(f"\nüìã Full Test Execution Output from {test_host}:")
                print("=" * 80)
                print(test_output)
                print("=" * 80)
                
                # Now execute on target host if different
                if test_host != target_host:
                    print("\n" + "=" * 80)
                    print(f"üöÄ FINAL EXECUTION: Running playbook on target host: {target_host}")
                    print("=" * 80)
                    print(f"\nüìç Executing on: {target_host}")
                    print()
                    
                    final_success, final_output = test_playbook_on_server(filename, target_host, check_mode=False, verbose=False, skip_debug=True)
                    
                    if final_success:
                        print("\n" + "=" * 80)
                        print(f"üéä COMPLETE SUCCESS! Playbook executed on target: {target_host}!")
                        print("=" * 80)
                        print("\n‚úÖ Final Execution Summary:")
                        print("   1. ‚úÖ Syntax check passed")
                        print(f"   2. ‚úÖ Test execution passed on {test_host}")
                        print(f"   3. ‚úÖ Final execution passed on {target_host}")
                        print("   4. ‚úÖ All requirements verified")
                        
                        # Show FULL final execution output
                        print(f"\nüìã Full Final Execution Output from {target_host}:")
                        print("=" * 80)
                        print(final_output)
                        print("=" * 80)
                    else:
                        print("\n" + "=" * 80)
                        print(f"‚ö†Ô∏è  Execution on target host {target_host} had issues")
                        print("=" * 80)
                        print(f"\nüìã Full Execution Output from {target_host}:")
                        print("=" * 80)
                        print(final_output)
                        print("=" * 80)
                        print("\n‚ö†Ô∏è  The playbook was validated on test host but may need adjustment for target host.")
                        print(f"   Test host: {test_host} ‚úÖ")
                        print(f"   Target host: {target_host} ‚ùå")
                else:
                    print("\n" + "=" * 80)
                    print(f"üéä COMPLETE SUCCESS! Playbook executed on {target_host}!")
                    print("=" * 80)
                
                break
            else:
                # Test failed
                if attempt < max_retries:
                    print(f"\n‚ö†Ô∏è  Server test failed on attempt {attempt}/{max_retries}")
                    print("üîÑ Retrying with test failure feedback to LLM...")
                    
                    # Check if it's an undefined variable bug
                    is_undefined_variable = "PLAYBOOK BUG" in test_output and "undefined" in test_output.lower()
                    
                    if is_undefined_variable:
                        # For undefined variable bugs, include more context
                        print("\nüìã Detected Undefined Variable Error - Providing detailed feedback to LLM")
                        
                        # Extract the specific error message
                        error_context = []
                        output_lines = test_output.split('\n')
                        for i, line in enumerate(output_lines):
                            if 'undefined' in line.lower() or 'PLAYBOOK BUG' in line:
                                # Get surrounding lines for context
                                start = max(0, i-5)
                                end = min(len(output_lines), i+10)
                                error_context = output_lines[start:end]
                                break
                        
                        if error_context:
                            error_msg = '\n'.join(error_context)
                        else:
                            error_msg = test_output[:500]
                        
                        # Escape curly braces
                        error_msg_escaped = error_msg.replace('{', '{{').replace('}', '}}')
                        
                        # Add specific undefined variable instructions
                        requirements.append(f"""CRITICAL FIX REQUIRED: Previous playbook had UNDEFINED VARIABLE error.
Error details: {error_msg_escaped}

To fix this:
1. Ensure ALL variables are defined BEFORE they are used
2. NEVER use a variable in the same set_fact task where it's being defined
3. Split set_fact tasks that have dependencies into separate tasks
4. Use 'variable_name | default(value)' for safety
5. Always set 'gather_facts: yes' at the playbook level

Example of the problem:
BAD - Circular reference:
  - set_fact:
      var1: "{{{{ some_value }}}}"
      var2: "{{{{ var1 | upper }}}}"  # ERROR: var1 doesn't exist yet!

GOOD - Split into separate tasks:
  - set_fact:
      var1: "{{{{ some_value }}}}"
  - set_fact:
      var2: "{{{{ var1 | upper }}}}"  # OK: var1 exists now""")
                    else:
                        # For other errors, use shorter context
                        test_output_escaped = test_output[:300].replace('{', '{{').replace('}', '}}')
                        requirements.append(f"IMPORTANT: Previous playbook failed testing: {test_output_escaped}")
                else:
                    print(f"\n‚ùå Failed to generate working playbook after {max_retries} attempts")
                    print(f"Last test output:\n{test_output}")
                    print(f"\n‚ö†Ô∏è  The playbook has been saved to: {filename}")
                    print("Please review and fix the issues manually.")
                    raise Exception(f"Playbook testing failed after {max_retries} attempts")
        
    except Exception as e:
        print(f"\n‚ùå Error generating playbook: {str(e)}")
        raise


if __name__ == "__main__":
    main()

