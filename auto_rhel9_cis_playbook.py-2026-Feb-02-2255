#!/usr/bin/env python3
"""
Automated CIS RHEL 9 Playbook Generator

This script:
1. Extracts all CIS checkpoint indices from the benchmark PDF
2. Queries the CIS RHEL 8 Benchmark using RAG to get checkpoint audit info
3. Uses DeepSeek AI to generate playbook requirements based on each checkpoint
4. Generates Ansible playbooks to audit all CIS checkpoints
5. Saves all playbooks to the specified output directory

Usage:
    python3 auto_rhel9_cis_playbook.py
    python3 auto_rhel9_cis_playbook.py --output-dir ./playbooks
    python3 auto_rhel9_cis_playbook.py --output-dir ./playbooks --target-host 192.168.122.16
"""

import os
import sys
import json
import re
import argparse
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# =============================================================================
# Checkpoint Index Extraction (from cis_index.py)
# =============================================================================

def read_checkpoint_indices_from_file(index_file_path: str) -> list:
    """
    Read checkpoint indices from a text file.
    
    Args:
        index_file_path: Path to the file containing checkpoint indices (one per line)
        
    Returns:
        list: List of checkpoint strings (e.g., ["1.1.1.1 Ensure...", "1.1.1.2 Ensure..."])
    """
    checkpoints = []
    
    try:
        with open(index_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # Skip empty lines and comments
                if line and not line.startswith('#'):
                    checkpoints.append(line)
        
        print(f"‚úÖ Read {len(checkpoints)} checkpoint indices from {index_file_path}")
        return checkpoints
    except FileNotFoundError:
        print(f"‚ùå Error: Index file not found: {index_file_path}")
        raise
    except Exception as e:
        print(f"‚ùå Error reading index file {index_file_path}: {e}")
        raise


def extract_checkpoint_indices(pdf_path: str) -> list:
    """
    Extract all CIS checkpoint indices from the benchmark PDF.
    
    Args:
        pdf_path: Path to the CIS benchmark PDF
        
    Returns:
        list: List of checkpoint strings (e.g., ["1.1.1.1 Ensure...", "1.1.1.2 Ensure..."])
    """
    from langchain_community.document_loaders import PyPDFLoader
    
    print(f"Loading PDF from {pdf_path}...")
    loader = PyPDFLoader(pdf_path)
    data = loader.load()
    print(f"Loaded {len(data)} pages from CIS benchmark document")
    
    # Patterns for extraction
    start_pattern = r"^Recommendations[\s.]+\d+$"
    end_pattern = r"^Appendix:\sSummary\sTable[\s.]+\d+$"
    section_start_regex = r"^\d+(\.\d+)+"  # Matches "6.3.3.19"
    strip_dots_pattern = r"\s*\.{2,}.*$"
    
    found_start = False
    buffer = ""
    extracted_items = []
    
    for doc in data:
        lines = doc.page_content.splitlines()
        
        for line in lines:
            clean_line = line.strip()
            
            # Skip empty lines
            if not clean_line:
                continue
            
            # Check for start marker
            if re.search(start_pattern, clean_line, re.IGNORECASE):
                found_start = True
                continue
            
            # Check for end marker
            if re.search(end_pattern, clean_line, re.IGNORECASE):
                found_start = False
                break
            
            # Extract checkpoints
            if found_start:
                # Skip noise
                if re.match(r"Page\s\d+", clean_line) or "Internal Only" in clean_line:
                    continue
                
                clean_line = re.sub(strip_dots_pattern, "", clean_line)
                clean_line = clean_line.strip()
                
                # Logic to handle wrapped lines
                if "(Automated)" in clean_line or "(Manual)" in clean_line:
                    if re.match(section_start_regex, clean_line):
                        final_text = clean_line
                    else:
                        final_text = buffer + " " + clean_line
                    extracted_items.append(final_text.strip())
                else:
                    if re.match(section_start_regex, clean_line):
                        buffer = clean_line
    
    print(f"‚úÖ Extracted {len(extracted_items)} checkpoint indices")
    return extracted_items


# =============================================================================
# Vector Store Setup (from all_cis_checkpoints_to_playbooks.py)
# =============================================================================

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Configuration
VECTOR_STORE_DIR = Path(__file__).parent / "CIS_RHEL8_DATA_DEEPSEEK"
PDF_PATH = "resources/CIS_Red_Hat_Enterprise_Linux_8_Benchmark_v4.0.0.pdf"

# Use HuggingFace embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

def load_or_create_vector_store():
    """Load existing vector store or create new one from PDF."""
    
    if (VECTOR_STORE_DIR / "chroma.sqlite3").exists():
        print(f"Loading existing vector store from {VECTOR_STORE_DIR}...")
        vector_store = Chroma(
            persist_directory=str(VECTOR_STORE_DIR),
            embedding_function=embeddings
        )
        doc_count = vector_store._collection.count()
        print(f"Vector store loaded with {doc_count} documents")
        
        if doc_count == 0:
            print("‚ö†Ô∏è  WARNING: Vector store exists but has 0 documents!")
            print("    Deleting empty vector store and recreating...")
            import shutil
            shutil.rmtree(VECTOR_STORE_DIR)
            VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)
        else:
            # Test search to verify it works
            try:
                test_results = vector_store.similarity_search("CIS benchmark", k=1)
                if test_results:
                    print(f"‚úÖ Vector store verified - test search returned results")
                else:
                    print("‚ö†Ô∏è  WARNING: Test search returned no results")
            except Exception as e:
                print(f"‚ö†Ô∏è  WARNING: Test search failed: {e}")
            return vector_store
    
    # Create new vector store from PDF
    print("No existing vector store found. Creating from PDF...")
    
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    print(f"Loading CIS RHEL 8 Benchmark PDF from {PDF_PATH}...")
    loader = PyPDFLoader(PDF_PATH)
    data = loader.load()
    print(f"Loaded {len(data)} pages from CIS benchmark document")
    
    # Increased chunk size and overlap to better handle long scripts
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000, chunk_overlap=500, add_start_index=True
    )
    all_splits = text_splitter.split_documents(data)
    print(f"Created {len(all_splits)} document chunks")
    
    print(f"Building and persisting vector store to {VECTOR_STORE_DIR}...")
    vector_store = Chroma.from_documents(
        documents=all_splits,
        embedding=embeddings,
        persist_directory=str(VECTOR_STORE_DIR)
    )
    print("Vector store created and saved")
    
    return vector_store


# =============================================================================
# CIS Checkpoint Search - Using Agent-based RAG
# =============================================================================

def create_cis_search_tool(vector_store):
    """Create a search tool for the CIS benchmark vector store."""
    from langchain_core.tools import tool
    
    @tool
    def search_cis_benchmark(query: str) -> str:
        """Search the CIS RHEL 8 Benchmark document for security control information.
        
        Use this tool to find:
        - Audit procedures for specific CIS controls
        - Remediation steps for security configurations
        - Profile applicability (Level 1/Level 2, Server/Workstation)
        - Rationale for security recommendations
        
        Args:
            query: The CIS control number (e.g., '1.1.1.1') or description to search for
        """
        # Increase k to 10 to ensure we get enough context for long scripts and procedures
        results = vector_store.similarity_search(query, k=10)
        # Combine top results for more comprehensive context
        combined_content = "\n\n---\n\n".join([doc.page_content for doc in results])
        return combined_content
    
    return search_cis_benchmark


def get_checkpoint_info_with_agent(vector_store, checkpoint: str, verbose: bool = False) -> str:
    """
    Use an agent-based RAG approach to get checkpoint information.
    
    Args:
        vector_store: The Chroma vector store
        checkpoint: The CIS checkpoint ID/description
        verbose: If True, print debug information
        
    Returns:
        str: Agent response with checkpoint information
    """
    from langchain_openai import ChatOpenAI
    from langchain.agents import create_agent
    from langchain_core.messages import HumanMessage
    
    # Create the search tool
    search_tool = create_cis_search_tool(vector_store)
    
    # Create the LLM - Use deepseek-chat for agentic tool use
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    # Create agent with system prompt optimized for extracting structured info
    system_prompt = """You are a CIS RHEL 8 security expert. Your task is to find and extract detailed information about a specific CIS checkpoint.

When asked about a checkpoint:
1. Use the search tool to find information about the checkpoint.
2. Search multiple times if needed - try different queries like:
   - The checkpoint number (e.g., "1.1.1.2")
   - The checkpoint with "Ensure" (e.g., "1.1.1.2 Ensure")
   - Keywords from the checkpoint (e.g., "freevxfs kernel module")
3. Extract ALL relevant information including:
   - Profile Applicability (Level 1/Level 2, Server/Workstation)
   - Description
   - Rationale
   - Audit procedure (exact commands)
   - Remediation procedure (exact commands)
   - Impact
   - Default Value

**CRITICAL INSTRUCTIONS FOR SCRIPTS:**
- If an audit or remediation procedure contains a bash script, you MUST extract the ENTIRE script.
- Do NOT truncate scripts. Ensure every line from the opening '{' to the closing '}' is included.
- If a script seems incomplete in one search result, search again or look at adjacent chunks to find the rest of it.
- The scripts are essential for the audit - missing even one line makes them useless.

Be thorough and search multiple times to find complete information."""

    agent = create_agent(
        model=llm,
        tools=[search_tool],
        system_prompt=system_prompt
    )
    
    # Query the agent for checkpoint information
    query = f"""Find and extract ALL information about CIS checkpoint {checkpoint}.

I need the following in your response (use the search tool multiple times if needed):
1. Checkpoint ID (exact number like 1.1.1.2)
2. Title (e.g., "Ensure freevxfs kernel module is not available")
3. Profile Applicability (Level 1 or Level 2, Server or Workstation)
4. Description (what this control does)
5. Rationale (why this is important for security)
6. COMPLETE Audit procedure (include ALL shell commands exactly as shown)
7. COMPLETE Remediation procedure (include ALL shell commands exactly as shown)
8. Impact (if any)
9. Default Value (if mentioned)

Search for "{checkpoint}" and related terms to find all the details."""

    if verbose:
        print(f"üîç DEBUG: Querying agent for checkpoint: {checkpoint}")
    
    response = agent.invoke(
        {"messages": [HumanMessage(content=query)]}
    )
    
    # Get the agent's response
    agent_response = response["messages"][-1].content
    
    if verbose:
        print(f"üîç DEBUG: Agent response length: {len(agent_response)} characters")
    
    return agent_response


def parse_agent_response_to_checkpoint_info(checkpoint: str, agent_response: str) -> dict:
    """
    Parse the agent's natural language response into a structured dictionary.
    
    Args:
        checkpoint: The original checkpoint query
        agent_response: The agent's response text
        
    Returns:
        dict: Structured checkpoint information
    """
    patterns = {
        "checkpoint_id": r"###\s*\d+\.\s*\*\*Checkpoint\s*ID\*\*",
        "title": r"###\s*\d+\.\s*\*\*Title.*?:?\s*\**",
        "profile_applicability": r"###\s*\d+\.\s*\*\*Profile\s*Applicability.*?:?\s*\**",
        "description": r"###\s*\d+\.\s*\*\*Description.*?:?\s*\**",
        "rationale": r"###\s*\d+\.\s*\*\*Rationale.*?:?\s*\**",
        "audit_procedure": r"###\s*\d+\.\s*\*\*COMPLETE\s+Audit\s+Procedure\*\*[:\s]*",
        "remediation_procedure": r"###\s*\d+\.\s*\*\*COMPLETE\s+Remediation\s+Procedure\*\*[:\s]*"
    }

    results = {}

    for key, header_regex in patterns.items():
        # Match the header, then capture everything until the next major section header or end of string
        full_pattern = rf"(?i){header_regex}(.*?)(?=\n###\s*\d+\.\s*\*\*|\Z)"
        
        match = re.search(full_pattern, agent_response, re.DOTALL)
        
        if not match and key in ["audit_procedure", "remediation_procedure"]:
            # Fallback for audit/remediation if "COMPLETE" prefix is missing
            simple_key = "Audit" if key == "audit_procedure" else "Remediation"
            fallback_regex = rf"###\s*\d+\.\s*\*\*{simple_key}.*?\*\*[:\s]*"
            full_pattern = rf"(?i){fallback_regex}(.*?)(?=\n###\s*\d+\.\s*\*\*|\Z)"
            match = re.search(full_pattern, agent_response, re.DOTALL)
        
        if match:
            content = match.group(1).strip()
            
            # Special handling for checkpoint_id
            if key == "checkpoint_id":
                id_match = re.search(r'\*\*(\d+\.\d+[\d\.]*)\*\*', content)
                if id_match:
                    content = id_match.group(1)
                else:
                    content = content.strip("*").strip()
                    id_match = re.search(r'(\d+\.\d+[\d\.]*)', content)
                    if id_match:
                        content = id_match.group(1)
                    else:
                        id_match = re.search(r'(\d+\.\d+[\d\.]*)', checkpoint)
                        if id_match:
                            content = id_match.group(1)
                        else:
                            content = None
            
            results[key] = content
        else:
            # Special fallback for checkpoint_id if not found with standard pattern
            if key == "checkpoint_id":
                alt_patterns = [
                    r'(?i)###\s*\*\*\s*\d+\.\s*Checkpoint\s*ID\s*\*\*\s*\n\s*(\d+\.\d+[\d\.]*)',
                    r'(?i)###\s*\d+\.\s*Checkpoint\s*ID\s*\n\s*\*\*(\d+\.\d+[\d\.]*)\*\*',
                    r'(?i)###\s*\d+\.\s*Checkpoint\s*ID\s*\n\s*(\d+\.\d+[\d\.]*)',
                    r'(?i)Checkpoint\s*ID[^\d]*\*\*(\d+\.\d+[\d\.]*)\*\*',
                ]
                
                for alt_pattern in alt_patterns:
                    alt_match = re.search(alt_pattern, agent_response)
                    if alt_match:
                        results[key] = alt_match.group(1)
                        break
                
                if key not in results:
                    id_match = re.search(r'(\d+\.\d+[\d\.]*)', checkpoint)
                    results[key] = id_match.group(1) if id_match else None
            else:
                results[key] = None

    return results


# =============================================================================
# Playbook Requirements Generation
# =============================================================================

def extract_audit_steps_from_procedure(audit_procedure: str, checkpoint_id: str = "", title: str = "") -> list:
    """
    Extract individual audit steps/commands/scripts from the CIS audit procedure text using DeepSeek AI.
    
    Args:
        audit_procedure: The complete audit procedure text from CIS benchmark
        checkpoint_id: The checkpoint ID for context
        title: The checkpoint title
        
    Returns:
        list: List of requirement strings extracted from the audit procedure
    """
    if not audit_procedure or len(audit_procedure) < 20:
        return []

    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    
    # Use deepseek-chat for better logic extraction
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    prompt_template = """You are a CIS RHEL 8 security expert. Analyze the following CIS Audit Procedure and extract individual audit requirements.

**Checkpoint ID:** {checkpoint_id}
**Title:** {title}

**Audit Procedure:**
{audit_procedure}

**Task:**
1. Break down the audit procedure into individual, actionable audit steps.
2. **MERGE related information**: If a step includes a description, a command, and an example output, they MUST be merged into a single requirement. Do not create separate requirements for the command and its example output.
3. **Analyze expected values**: Carefully look for phrases like "is set to", "should be", "Example output:", or "verify... is...". Use these to determine the EXACT expected value.
4. **Phrase requirements in terms of expected/desired state**: Requirements should reflect what we're VERIFYING (the desired/expected state), not just what we're checking.
   - If the audit procedure says "if nothing is returned, PASS" or "if output is empty, PASS" or "if nothing is returned, no further audit steps are required", the requirement should be phrased as "verify the module does NOT exist" or "verify the module is NOT available", NOT "check if module exists".
   - If the audit procedure says "if output shows X, PASS", the requirement should be phrased as "verify that X is present/configured", NOT "check if X exists".
   - Examples:
     * WRONG: "Run script to check if cramfs module exists in filesystem"
     * CORRECT: "Run script to verify the cramfs module does NOT exist in filesystem"
     * WRONG: "Check if service is running"
     * CORRECT: "Verify the service is running" (if running is the expected state)
     * WRONG: "Check if weak ciphers are present"
     * CORRECT: "Verify weak ciphers are NOT present" (if absence is the expected state)
5. **Specific Rationales**: Do NOT use generic rationales like "PASS when command shows compliant state". Instead, be specific.
   - Example: "Rationale: PASS when output is 'kernel.yama.ptrace_scope = 1', '2', or '3', FAIL otherwise."
   - Example: "Rationale: PASS when the script successfully verifies the configuration in sysctl files, FAIL if incorrect values are found."
6. **Complex Logic**: Identify any logical dependencies (AND/OR) between steps.
7. **Requirement Format**: Each requirement MUST follow this format:
   "Description of what to verify (expected state) using command: `<command>`. Rationale: PASS when <specific condition>, FAIL when <specific condition>"
8. **Bash Scripts**: If the procedure includes a bash script, include the FULL script content:
   "Run the following script to verify <expected state>: ```#!/usr/bin/env bash\\n<script content>```. Rationale: PASS when <condition>, FAIL when <condition>"
9. **OVERALL Verify**: The LAST requirement MUST be an "OVERALL Verify" that combines the results of all previous requirements.
   Example: "OVERALL Verify: <checkpoint title>. Rationale: PASS when req_1=PASS AND req_2=PASS, FAIL otherwise"

**Output Format:**
Return ONLY a JSON list of strings. No markdown code blocks, no explanations.
Example:
[
  "Verify kernel.yama.ptrace_scope in running config using command: `sysctl kernel.yama.ptrace_scope`. Rationale: PASS when output is 'kernel.yama.ptrace_scope = 1', '2', or '3', FAIL otherwise",
  "OVERALL Verify: Ensure ptrace_scope is restricted. Rationale: PASS when req_1=PASS, FAIL otherwise"
]

Generate the JSON list now:"""

    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm
    
    try:
        response = chain.invoke({
            'checkpoint_id': checkpoint_id,
            'title': title,
            'audit_procedure': audit_procedure
        })
        
        response_text = response.content.strip()
        
        # Clean up response - handle potential extra text before/after JSON
        json_match = re.search(r'\[\s*".*"\s*\]', response_text, re.DOTALL)
        if json_match:
            try:
                requirements = json.loads(json_match.group(0))
                if isinstance(requirements, list):
                    return requirements
            except json.JSONDecodeError:
                pass

        # Fallback cleanup
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
            
        try:
            requirements = json.loads(response_text)
            if isinstance(requirements, list):
                return requirements
        except json.JSONDecodeError:
            return []
            
        return []
            
    except Exception as e:
        print(f"    ‚ö†Ô∏è AI requirement extraction failed: {e}")
        return []


def generate_playbook_requirements_from_checkpoint(checkpoint_info: dict) -> dict:
    """
    Generate playbook requirements based on CIS checkpoint info.
    
    Args:
        checkpoint_info: Dict containing checkpoint details
        
    Returns:
        dict: {
            'objective': str,
            'requirements': list[str]
        }
    """
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    
    checkpoint_id = checkpoint_info.get('checkpoint_id', 'Unknown')
    title = checkpoint_info.get('title', '')
    
    # Get raw agent response if available
    raw_agent_response = checkpoint_info.get('raw_agent_response', '')
    audit_procedure = checkpoint_info.get('audit_procedure', '')
    
    # FIRST: Try to extract requirements directly from audit procedure
    source_text = audit_procedure if audit_procedure and len(audit_procedure) > 100 else raw_agent_response
    
    extracted_requirements = extract_audit_steps_from_procedure(source_text, checkpoint_id, title)
    
    if extracted_requirements and len(extracted_requirements) >= 2:
        return {
            'objective': f"Audit CIS checkpoint {checkpoint_id}: {title}" if title else f"Audit CIS checkpoint: {checkpoint_id}",
            'requirements': extracted_requirements
        }
    
    # FALLBACK: Use LLM to generate requirements
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    # Build the prompt
    additional_context = ""
    if raw_agent_response:
        additional_context = f"""
**Full Agent Response (contains complete checkpoint details):**
{raw_agent_response[:6000]}
"""
    
    prompt_template = """You are an expert system administrator creating Ansible playbooks for CIS benchmark compliance auditing.

Based on the following CIS checkpoint information, generate:
1. A clear playbook objective (one sentence) focused on AUDITING this security control
2. A list of specific requirements for an Ansible playbook

**CIS Checkpoint Information:**
- Checkpoint ID: {checkpoint_id}
- Title: {title}
- Profile Applicability: {profile_applicability}
- Description: {description}
- Rationale: {rationale}

**Audit Procedure from CIS Benchmark:**
{audit_procedure}

**Remediation Procedure from CIS Benchmark:**
{remediation_procedure}
{additional_context}
**Task:**
Generate Ansible playbook requirements that will:
1. AUDIT the system to check if it complies with this CIS checkpoint
2. COLLECT the current system state/configuration
3. COMPARE against the expected values from CIS benchmark
4. REPORT compliance status (PASS/FAIL) with details

**MERGE related information**: If a step includes a description, a command, and an example output, they MUST be merged into a single requirement. Do not create separate requirements for the command and its example output.

**Analyze expected values**: Carefully look for phrases like "is set to", "should be", "Example output:", or "verify... is...". Use these to determine the EXACT expected value.

**Phrase requirements in terms of expected/desired state**: Requirements should reflect what we're VERIFYING (the desired/expected state), not just what we're checking.
   - If the audit procedure says "if nothing is returned, PASS" or "if output is empty, PASS", the requirement should be phrased as "verify the module does NOT exist" or "verify the module is NOT available", NOT "check if module exists".
   - If the audit procedure says "if output shows X, PASS", the requirement should be phrased as "verify that X is present/configured", NOT "check if X exists".
   - Examples:
     * WRONG: "Run script to check if cramfs module exists in filesystem"
     * CORRECT: "Run script to verify the cramfs module does NOT exist in filesystem"
     * WRONG: "Check if service is running"
     * CORRECT: "Verify the service is running" (if running is the expected state)
     * WRONG: "Check if weak ciphers are present"
     * CORRECT: "Verify weak ciphers are NOT present" (if absence is the expected state)

**Specific Rationales**: Do NOT use generic rationales like "PASS when command shows compliant state". Instead, be specific.
   - Example: "Rationale: PASS when output is 'kernel.yama.ptrace_scope = 1', '2', or '3', FAIL otherwise."

**Output Format:**
Return ONLY a valid JSON object with this exact structure (no markdown, no code blocks):
{{
    "objective": "Audit CIS checkpoint {checkpoint_id}: {title}",
    "requirements": [
        "Verify <expected state> using command: `<command>`. Rationale: PASS when <expected result>, FAIL when <failure condition>",
        "Verify <setting> is <expected value> with command: `<command>`. Rationale: PASS when <expected>, FAIL otherwise",
        "Run the following audit script to verify <module> does NOT exist: ```#!/usr/bin/env bash\\n<full script content here>```. Rationale: PASS when script returns no output (module not available), FAIL when output indicates module exists",
        ...
    ]
}}

**Script Inclusion Example:**
If the audit procedure contains a script like:
  #!/usr/bin/env bash
  l_output="" l_output2=""
  ...
And the audit procedure says "if nothing is returned, PASS" or "if output is empty, PASS" or "if nothing is returned, no further audit steps are required", then the requirement should be:
  "Run the following audit script to verify the kernel module does NOT exist: ```#!/usr/bin/env bash\\nl_output=\"\" l_output2=\"\"\\n...full script...```. Rationale: PASS when script returns no output (module not available), FAIL when output indicates module exists"
  
NOT: "Run the following audit script to check if kernel module exists: ..." (this is WRONG - it doesn't reflect the expected state)

**Important Guidelines:**
- Base requirements DIRECTLY on the audit procedure commands from the checkpoint information
- Include the exact commands from the CIS benchmark audit procedure
- Focus on AUDITING (checking compliance), not remediation
- Each requirement should map to a specific audit step
- CRITICAL: Each requirement MUST end with "Rationale: PASS when <condition>, FAIL when <condition>" explaining the pass/fail logic
- Include expected values/outputs for comparison
- If the Full Agent Response is provided, extract the audit commands from there
- CRITICAL: If the audit procedure contains a SCRIPT (bash script, shell script), you MUST include the FULL script content in the requirement, NOT just "use the provided script". The requirement must be SELF-CONTAINED with all script details.
- When including scripts, format them as: "Run the following script: ```<full script content>```"
- Never reference "the audit script" or "the provided script" without including the actual script code
- The LAST requirement MUST be an "OVERALL Verify" that combines the results of all previous requirements.
   Example: "OVERALL Verify: <checkpoint title>. Rationale: PASS when req_1=PASS AND req_2=PASS, FAIL otherwise"

Generate the JSON now:"""

    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm
    
    response = chain.invoke({
        'checkpoint_id': checkpoint_info.get('checkpoint_id', 'Unknown'),
        'title': checkpoint_info.get('title', '') or 'Unknown',
        'profile_applicability': checkpoint_info.get('profile_applicability', '') or 'Not specified',
        'description': checkpoint_info.get('description', '') or 'Not specified',
        'additional_context': additional_context,
        'rationale': checkpoint_info.get('rationale', '') or 'See Full Agent Response above',
        'audit_procedure': checkpoint_info.get('audit_procedure', '') or 'See Full Agent Response above',
        'remediation_procedure': checkpoint_info.get('remediation_procedure', '') or 'See Full Agent Response above'
    })
    
    response_text = response.content.strip()
    
    # Clean up response
    json_match = re.search(r'\{\s*".*"\s*\}', response_text, re.DOTALL)
    if json_match:
        try:
            result = json.loads(json_match.group(0))
            if isinstance(result, dict) and 'requirements' in result:
                return result
        except json.JSONDecodeError:
            pass

    # Fallback cleanup
    if "```json" in response_text:
        response_text = response_text.split("```json")[1].split("```")[0].strip()
    elif "```" in response_text:
        response_text = response_text.split("```")[1].split("```")[0].strip()
    
    try:
        result = json.loads(response_text)
        return result
    except json.JSONDecodeError:
        # Build fallback requirements
        checkpoint_id = checkpoint_info.get('checkpoint_id', 'Unknown')
        title = checkpoint_info.get('title', '')
        audit_proc = checkpoint_info.get('audit_procedure', '')
        raw_response = checkpoint_info.get('raw_agent_response', '')
        
        source_text = audit_proc if audit_proc and len(audit_proc) > 50 else raw_response
        fallback_requirements = []
        
        if source_text:
            cmd_patterns = [
                r'(?:Run|Execute|Use)[:\s]+[`\'"]?([^`\'"]+(?:lsmod|modprobe|find|grep|cat|ls|systemctl|sysctl)[^`\'"]*)[`\'"]?',
                r'#\s*([a-z/]+\s+[^\n]+)',
                r'```(?:bash|shell)?\n([^`]+)```',
            ]
            
            for pattern in cmd_patterns:
                matches = re.findall(pattern, source_text, re.IGNORECASE)
                for match in matches[:5]:
                    cmd = match.strip()
                    if len(cmd) > 10 and len(cmd) < 200:
                        fallback_requirements.append(f"Execute audit command: `{cmd}`. Rationale: PASS when command shows compliant state, FAIL otherwise")
        
        if not fallback_requirements:
            if title:
                fallback_requirements.append(f"Verify that: {title}. Rationale: PASS when verified, FAIL otherwise")
            fallback_requirements.extend([
                f"Check system configuration for CIS {checkpoint_id}. Rationale: PASS when compliant, FAIL otherwise",
                "Collect audit evidence and system state. Rationale: PASS when data collected, FAIL otherwise"
            ])
        
        return {
            'objective': f"Audit CIS checkpoint {checkpoint_id}: {title}" if title else f"Audit CIS checkpoint: {checkpoint_id}",
            'requirements': fallback_requirements
        }


def run_playbook_generation(objective, requirements, target_host, test_host, become_user, filename, skip_execution=True, audit_procedure=None):
    """
    Call langgraph_deepseek_generate_playbook to generate and execute the playbook.
    
    Args:
        objective: Playbook objective
        requirements: List of requirements
        target_host: Target host for execution
        test_host: Test host for validation
        become_user: User to become
        filename: Output filename
        skip_execution: If True, skip playbook execution
        audit_procedure: CIS Benchmark audit procedure (script/commands)
        
    Returns:
        tuple: (success: bool, message: str)
    """
    max_retries = max(len(requirements), 3)
    
    try:
        # Import the workflow function
        from langgraph_deepseek_generate_playbook import generate_playbook_workflow
    
        # Call the workflow programmatically
        final_state = generate_playbook_workflow(
            objective=objective,
            requirements=requirements,
            target_host=target_host,
            test_host=test_host,
            become_user=become_user,
            filename=filename,
            audit_procedure=audit_procedure,
            max_retries=max_retries,
            verbose=False  # Set to False for automated mode
        )
        
        # Check if successful
        if final_state['workflow_complete'] and final_state['test_success']:
            return True, "Playbook generation and execution completed successfully"
        else:
            return False, f"Workflow failed: {final_state.get('error_message', 'Unknown error')}"
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return False, f"Error running playbook generation: {str(e)}"


# =============================================================================
# Automated Checkpoint Processing
# =============================================================================

def process_checkpoint_automated(vector_store, checkpoint: str, output_dir: Path, target_host: str, 
                                  test_host: str, become_user: str, skip_execution: bool, 
                                  verbose: bool = False) -> dict:
    """
    Process a single CIS checkpoint and generate playbook (automated, no user input).
    
    Returns:
        dict: {
            'checkpoint': str,
            'success': bool,
            'filename': str,
            'error': str or None
        }
    """
    result = {
        'checkpoint': checkpoint,
        'success': False,
        'filename': None,
        'error': None
    }
    
    try:
        # Step 1: Use Agent-based RAG to get checkpoint info
        if verbose:
            print(f"\n{'='*100}")
            print(f"Querying CIS RHEL 8 Benchmark using Agent RAG for: '{checkpoint}'")
            print(f"{'='*100}")
        
        agent_response = get_checkpoint_info_with_agent(vector_store, checkpoint, verbose=verbose)
        
        # Step 2: Parse the agent response into structured format
        checkpoint_info = parse_agent_response_to_checkpoint_info(checkpoint, agent_response)
        checkpoint_info['raw_agent_response'] = agent_response
        
        checkpoint_id = checkpoint_info.get('checkpoint_id', '0.0.0.0')
        if not checkpoint_id or checkpoint_id in ['None', 'N/A', '']:
            # Extract from original checkpoint string
            id_match = re.search(r'(\d+\.\d+[\d\.]*)', checkpoint)
            checkpoint_id = id_match.group(1) if id_match else 'unknown'
        
        # Step 3: Generate playbook requirements
        playbook_spec = generate_playbook_requirements_from_checkpoint(checkpoint_info)
        
        objective = playbook_spec.get('objective', '')
        requirements = playbook_spec.get('requirements', [])
        
        # Add CIS reference to requirements
        requirements.append(f"Add comment referencing CIS RHEL 8 Benchmark v4.0.0, checkpoint {checkpoint_id}")
        requirements.append(f"""Create a task named 'Generate compliance report' that displays a debug msg with this EXACT format:
========================================================
        COMPLIANCE REPORT - CIS {checkpoint_id}
========================================================
Reference: CIS RHEL 8 Benchmark v4.0.0 checkpoint {checkpoint_id}
========================================================

REQUIREMENT 1 - <requirement description>:
  Task: <task name>
  Command: <command executed>
  Exit code: <exit code>
  Data: <command output>
  Status: PASS or FAIL
  Rationale: <why PASS or FAIL based on the requirement's rationale>

(repeat for each requirement)

========================================================
OVERALL COMPLIANCE:
  Result: PASS or FAIL
  Rationale: <overall pass/fail logic explanation>
========================================================

Each requirement MUST have Status and Rationale lines. The OVERALL COMPLIANCE section is REQUIRED at the end.""")
        requirements.append("CRITICAL: Use ignore_errors: true and failed_when: false on all audit tasks so all checks complete and report status")
        
        # Print requirements before generating playbook
        print(f"\n{'='*100}")
        print(f"üìã Generated Playbook Requirements for {checkpoint_id}")
        print(f"{'='*100}")
        print(f"Objective: {objective}")
        print(f"\nRequirements ({len(requirements)} items):")
        for idx, req in enumerate(requirements, 1):
            # Truncate very long requirements for display
            display_req = req[:200] + '...' if len(req) > 200 else req
            print(f"  {idx}. {display_req}")
        print(f"{'='*100}\n")
        
        # Step 4: Generate playbook filename
        safe_checkpoint_id = checkpoint_id.replace('.', '_')
        base_filename = f"cis_audit_{safe_checkpoint_id}.yml"
        filename = str(output_dir / base_filename)
        result['filename'] = filename
        
        # Get the audit procedure from checkpoint info
        audit_procedure = checkpoint_info.get('audit_procedure', '')
        raw_response = checkpoint_info.get('raw_agent_response', '')
        
        # If audit_procedure is empty or generic, try to extract from raw_agent_response
        if not audit_procedure or audit_procedure in ['Not specified', 'See Full Agent Response above', 'N/A', '']:
            if raw_response:
                audit_patterns = [
                    r'\*\*Audit(?:\s+Procedure)?\*\*[:\s]*\n?(.*?)(?=\*\*(?:Remediation|Impact|Default|References)|\n\n\*\*|$)',
                    r'(?:Audit|AUDIT)(?:\s+Procedure)?[:\s]*\n(.*?)(?=(?:Remediation|REMEDIATION|Impact|IMPACT|Default|DEFAULT|References|$))',
                    r'\d+\.\s*(?:Audit|AUDIT)[^\n]*\n(.*?)(?=\d+\.\s*(?:Remediation|Impact)|$)',
                ]
                
                for pattern in audit_patterns:
                    audit_match = re.search(pattern, raw_response, re.DOTALL | re.IGNORECASE)
                    if audit_match and len(audit_match.group(1).strip()) > 50:
                        audit_procedure = audit_match.group(1).strip()
                        break
                
                if not audit_procedure or len(audit_procedure) < 50:
                    audit_procedure = raw_response
        
        # Step 5: Generate and optionally execute playbook
        success, output = run_playbook_generation(
            objective=objective,
            requirements=requirements,
            target_host=target_host,
            test_host=test_host if test_host else target_host,
            become_user=become_user,
            filename=filename,
            skip_execution=skip_execution,
            audit_procedure=audit_procedure if audit_procedure and len(audit_procedure) > 50 else None
        )
        
        result['success'] = success
        if not success:
            result['error'] = output
        
        return result
        
    except Exception as e:
        import traceback
        result['error'] = f"{str(e)}\n{traceback.format_exc()}"
        return result


def log_failed_checkpoint(checkpoint: str, error: str, log_file: Path):
    """
    Log a failed checkpoint to the failed_playbooks.log file.
    
    Args:
        checkpoint: The checkpoint string that failed
        error: The error message
        log_file: Path to the log file
    """
    try:
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"{'='*80}\n")
            f.write(f"Checkpoint: {checkpoint}\n")
            f.write(f"Error: {error}\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write(f"{'='*80}\n\n")
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Failed to write to log file {log_file}: {e}")


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Automated CIS RHEL 9 Playbook Generator - Generates playbooks for all CIS checkpoints',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Interactive output directory prompt
  python3 auto_rhel9_cis_playbook.py
  
  # Specify output directory
  python3 auto_rhel9_cis_playbook.py --output-dir ./playbooks
  
  # With custom target host
  python3 auto_rhel9_cis_playbook.py --output-dir ./playbooks --target-host 192.168.122.16
  
  # Skip execution (generate only)
  python3 auto_rhel9_cis_playbook.py --output-dir ./playbooks --skip-execution
  
  # Use checkpoint indices from file (skips PDF extraction)
  python3 auto_rhel9_cis_playbook.py --output-dir ./playbooks --index-file checkpoints.txt
"""
    )
    
    parser.add_argument(
        '--output-dir', '-d',
        type=str,
        default=None,
        help='Output directory for generated playbooks (will prompt if not provided)'
    )
    
    parser.add_argument(
        '--target-host', '-t',
        type=str,
        default='192.168.122.16',
        help='Target host for playbook execution (default: 192.168.122.16)'
    )
    
    parser.add_argument(
        '--test-host',
        type=str,
        default=None,
        help='Test host for validation before target execution'
    )
    
    parser.add_argument(
        '--become-user', '-u',
        type=str,
        default='root',
        help='User to become when executing tasks (default: root)'
    )
    
    parser.add_argument(
        '--skip-execution',
        action='store_true',
        help='Generate playbook but skip execution'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Show verbose output including raw search results for debugging'
    )
    
    parser.add_argument(
        '--pdf-path',
        type=str,
        default=None,
        help='Path to CIS benchmark PDF (default: resources/CIS_Red_Hat_Enterprise_Linux_8_Benchmark_v4.0.0.pdf)'
    )
    
    parser.add_argument(
        '--index-file', '-i',
        type=str,
        default=None,
        help='Path to file containing checkpoint indices (one per line). If provided, skips PDF extraction and uses this file instead.'
    )
    
    args = parser.parse_args()
    
    try:
        # Get output directory (required user input)
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            output_dir_str = input("\nEnter output directory for playbooks (e.g., ./playbooks): ").strip()
            if not output_dir_str:
                print("‚ùå Output directory is required. Exiting.")
                sys.exit(1)
            output_dir = Path(output_dir_str)
        
        # Create output directory if it doesn't exist
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"‚úÖ Output directory: {output_dir.absolute()}")
        
        # Initialize failed playbooks log file
        failed_log_file = output_dir / "failed_playbooks.log"
        # Clear or create the log file
        if failed_log_file.exists():
            failed_log_file.unlink()
        failed_log_file.touch()
        print(f"üìù Failed playbooks will be logged to: {failed_log_file.absolute()}")
        
        # Load vector store
        print("\n" + "="*100)
        print("üîß Initializing CIS RHEL 8 Benchmark Vector Store")
        print("="*100)
        
        vector_store = load_or_create_vector_store()
        print("‚úÖ Vector store ready")
        
        # Get checkpoint indices - either from file or extract from PDF
        if args.index_file:
            # Read checkpoint indices from provided file
            print("\n" + "="*100)
            print("üìã Reading CIS Checkpoint Indices from File")
            print("="*100)
            checkpoints = read_checkpoint_indices_from_file(args.index_file)
        else:
            # Extract checkpoint indices from PDF
            print("\n" + "="*100)
            print("üìã Extracting CIS Checkpoint Indices from PDF")
            print("="*100)
            pdf_path = args.pdf_path or PDF_PATH
            checkpoints = extract_checkpoint_indices(pdf_path)
        
        if not checkpoints:
            print("‚ùå No checkpoints found. Exiting.")
            sys.exit(1)
        
        print(f"\n‚úÖ Found {len(checkpoints)} checkpoints to process")
        
        # Process all checkpoints
        print("\n" + "="*100)
        print("üöÄ Starting Automated Playbook Generation")
        print("="*100)
        print(f"Total checkpoints: {len(checkpoints)}")
        print(f"Output directory: {output_dir.absolute()}")
        print(f"Target host: {args.target_host}")
        print(f"Skip execution: {args.skip_execution}")
        print("="*100)
        
        results = []
        successful = 0
        failed = 0
        
        for idx, checkpoint in enumerate(checkpoints, 1):
            print(f"\n{'='*100}")
            print(f"Processing checkpoint {idx}/{len(checkpoints)}: {checkpoint[:80]}...")
            print(f"{'='*100}")
            
            result = process_checkpoint_automated(
                vector_store=vector_store,
                checkpoint=checkpoint,
                output_dir=output_dir,
                target_host=args.target_host,
                test_host=args.test_host,
                become_user=args.become_user,
                skip_execution=args.skip_execution,
                verbose=args.verbose
            )
            
            results.append(result)
            
            if result['success']:
                successful += 1
                print(f"‚úÖ Success: {result['filename']}")
            else:
                failed += 1
                print(f"‚ùå Failed: {result['error']}")
                # Log failed checkpoint to file
                log_failed_checkpoint(
                    checkpoint=result['checkpoint'],
                    error=result['error'],
                    log_file=failed_log_file
                )
            
            # Progress summary
            print(f"\nProgress: {idx}/{len(checkpoints)} | Successful: {successful} | Failed: {failed}")
        
        # Final summary
        print("\n" + "="*100)
        print("üìä FINAL SUMMARY")
        print("="*100)
        print(f"Total checkpoints processed: {len(checkpoints)}")
        print(f"‚úÖ Successful: {successful}")
        print(f"‚ùå Failed: {failed}")
        print(f"üìÅ Output directory: {output_dir.absolute()}")
        print(f"üìù Failed checkpoints log: {failed_log_file.absolute()}")
        
        if failed > 0:
            print("\nFailed checkpoints (also logged to failed_playbooks.log):")
            for result in results:
                if not result['success']:
                    print(f"  - {result['checkpoint']}: {result['error'][:100]}...")
        
        print("\n‚úÖ Automated playbook generation completed!")
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

