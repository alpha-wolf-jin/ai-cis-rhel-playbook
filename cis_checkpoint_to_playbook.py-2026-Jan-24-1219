#!/usr/bin/env python3
"""
CIS Checkpoint to Ansible Playbook Generator

This script:
1. Queries the CIS RHEL 8 Benchmark using RAG to get checkpoint audit info
2. Uses DeepSeek AI to generate playbook requirements based on the checkpoint
3. Generates an Ansible playbook to audit the CIS checkpoint
4. Optionally executes the playbook on the target host

Usage:
    python3 cis_checkpoint_to_playbook.py --checkpoint "1.1.1.1"
    python3 cis_checkpoint_to_playbook.py --checkpoint "1.1.1.1 Ensure cramfs kernel module is not available" --target-host 192.168.122.16
"""

import os
import sys
import json
import argparse
from pprint import pprint
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# =============================================================================
# Vector Store Setup (from cis_rhel8_rag_deepseek.py)
# =============================================================================

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Configuration - Use same directory as cis_rhel8_rag_deepseek.py
VECTOR_STORE_DIR = Path(__file__).parent / "CIS_RHEL8_DATA_DEEPSEEK"
PDF_PATH = "resources/CIS_Red_Hat_Enterprise_Linux_8_Benchmark_v4.0.0.pdf"

# Use HuggingFace embeddings (same as cis_rhel8_rag_deepseek.py)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

def load_or_create_vector_store():
    """Load existing vector store or create new one from PDF."""
    
    if (VECTOR_STORE_DIR / "chroma.sqlite3").exists():
        print(f"Loading existing vector store from {VECTOR_STORE_DIR}...")
        vector_store = Chroma(
            persist_directory=str(VECTOR_STORE_DIR),
            embedding_function=embeddings
        )
        doc_count = vector_store._collection.count()
        print(f"Vector store loaded with {doc_count} documents")
        
        if doc_count == 0:
            print("‚ö†Ô∏è  WARNING: Vector store exists but has 0 documents!")
            print("    Deleting empty vector store and recreating...")
            import shutil
            shutil.rmtree(VECTOR_STORE_DIR)
            VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)
        else:
            # Test search to verify it works
            try:
                test_results = vector_store.similarity_search("CIS benchmark", k=1)
                if test_results:
                    print(f"‚úÖ Vector store verified - test search returned results")
                else:
                    print("‚ö†Ô∏è  WARNING: Test search returned no results")
            except Exception as e:
                print(f"‚ö†Ô∏è  WARNING: Test search failed: {e}")
            return vector_store
    
    # Create new vector store from PDF
    print("No existing vector store found. Creating from PDF...")
    
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    print(f"Loading CIS RHEL 8 Benchmark PDF from {PDF_PATH}...")
    loader = PyPDFLoader(PDF_PATH)
    data = loader.load()
    print(f"Loaded {len(data)} pages from CIS benchmark document")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500, chunk_overlap=300, add_start_index=True
    )
    all_splits = text_splitter.split_documents(data)
    print(f"Created {len(all_splits)} document chunks")
    
    print(f"Building and persisting vector store to {VECTOR_STORE_DIR}...")
    vector_store = Chroma.from_documents(
        documents=all_splits,
        embedding=embeddings,
        persist_directory=str(VECTOR_STORE_DIR)
    )
    print("Vector store created and saved")
    
    return vector_store


# =============================================================================
# CIS Checkpoint Search - Using Agent-based RAG (like cis_rhel8_rag_deepseek.py)
# =============================================================================

def create_cis_search_tool(vector_store):
    """Create a search tool for the CIS benchmark vector store."""
    from langchain_core.tools import tool
    
    @tool
    def search_cis_benchmark(query: str) -> str:
        """Search the CIS RHEL 8 Benchmark document for security control information.
        
        Use this tool to find:
        - Audit procedures for specific CIS controls
        - Remediation steps for security configurations
        - Profile applicability (Level 1/Level 2, Server/Workstation)
        - Rationale for security recommendations
        
        Args:
            query: The CIS control number (e.g., '1.1.1.1') or description to search for
        """
        results = vector_store.similarity_search(query, k=4)
        # Combine top results for more comprehensive context
        combined_content = "\n\n---\n\n".join([doc.page_content for doc in results])
        return combined_content
    
    return search_cis_benchmark


def get_checkpoint_info_with_agent(vector_store, checkpoint: str, verbose: bool = False) -> dict:
    """
    Use an agent-based RAG approach to get checkpoint information.
    This mirrors the approach in cis_rhel8_rag_deepseek.py which works better.
    
    Args:
        vector_store: The Chroma vector store
        checkpoint: The CIS checkpoint ID/description
        verbose: If True, print debug information
        
    Returns:
        dict: Structured checkpoint information
    """
    from langchain_openai import ChatOpenAI
    from langchain.agents import create_agent
    from langchain_core.messages import HumanMessage
    
    # Create the search tool
    search_tool = create_cis_search_tool(vector_store)
    
    # Create the LLM
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    # Create agent with system prompt optimized for extracting structured info
    system_prompt = """You are a CIS RHEL 8 security expert. Your task is to find and extract detailed information about a specific CIS checkpoint.

When asked about a checkpoint:
1. Use the search tool to find information about the checkpoint
2. Search multiple times if needed - try different queries like:
   - The checkpoint number (e.g., "1.1.1.2")
   - The checkpoint with "Ensure" (e.g., "1.1.1.2 Ensure")
   - Keywords from the checkpoint (e.g., "freevxfs kernel module")
3. Extract ALL relevant information including:
   - Profile Applicability (Level 1/Level 2, Server/Workstation)
   - Description
   - Rationale
   - Audit procedure (exact commands)
   - Remediation procedure (exact commands)
   - Impact
   - Default Value

Be thorough and search multiple times to find complete information."""

    agent = create_agent(
        model=llm,
        tools=[search_tool],
        system_prompt=system_prompt
    )
    
    # Query the agent for checkpoint information
    query = f"""Find and extract ALL information about CIS checkpoint {checkpoint}.

I need the following in your response (use the search tool multiple times if needed):
1. Checkpoint ID (exact number like 1.1.1.2)
2. Title (e.g., "Ensure freevxfs kernel module is not available")
3. Profile Applicability (Level 1 or Level 2, Server or Workstation)
4. Description (what this control does)
5. Rationale (why this is important for security)
6. COMPLETE Audit procedure (include ALL shell commands exactly as shown)
7. COMPLETE Remediation procedure (include ALL shell commands exactly as shown)
8. Impact (if any)
9. Default Value (if mentioned)

Search for "{checkpoint}" and related terms to find all the details."""

    if verbose:
        print(f"üîç DEBUG: Querying agent for checkpoint: {checkpoint}")
    
    response = agent.invoke(
        {"messages": [HumanMessage(content=query)]}
    )
    
    # Get the agent's response
    agent_response = response["messages"][-1].content
    
    if verbose:
        print(f"üîç DEBUG: Agent response length: {len(agent_response)} characters")
        print("\n" + "-"*50 + " AGENT RESPONSE " + "-"*50)
        print(agent_response[:3000] if len(agent_response) > 3000 else agent_response)
        print("-"*120 + "\n")
    
    return agent_response


def search_cis_checkpoint(vector_store, checkpoint: str, verbose: bool = False) -> str:
    """
    Search the CIS RHEL 8 Benchmark for checkpoint information.
    Falls back to direct search if agent approach fails.
    
    Args:
        vector_store: The Chroma vector store
        checkpoint: The CIS control number or description
        verbose: If True, print raw search results for debugging
        
    Returns:
        str: Combined content from relevant documents
    """
    import re
    
    # Extract checkpoint ID if present (e.g., "1.1.1.1" from "1.1.1.1 Ensure cramfs...")
    checkpoint_id_match = re.match(r'^(\d+\.\d+\.\d+\.?\d*)', checkpoint)
    checkpoint_id = checkpoint_id_match.group(1) if checkpoint_id_match else checkpoint
    
    # Build multiple search queries for better coverage
    enhanced_queries = []
    
    # Primary queries - most specific
    if checkpoint_id_match:
        enhanced_queries.extend([
            f"{checkpoint_id} Ensure",
            f"checkpoint {checkpoint_id}",
            f"{checkpoint_id} kernel module",
            f"{checkpoint_id} freevxfs",  # Common for 1.1.1.x
            f"{checkpoint_id} Audit",
            f"{checkpoint_id} Remediation",
        ])
    
    # Add the original query
    enhanced_queries.append(checkpoint)
    
    # Generic CIS-related queries
    enhanced_queries.extend([
        f"CIS {checkpoint_id}" if checkpoint_id_match else f"CIS {checkpoint}",
        f"Profile Applicability {checkpoint_id}" if checkpoint_id_match else checkpoint,
    ])
    
    # Remove duplicates while preserving order
    seen = set()
    unique_queries = []
    for q in enhanced_queries:
        if q not in seen:
            seen.add(q)
            unique_queries.append(q)
    
    if verbose:
        print(f"\nüîç DEBUG: Checkpoint ID extracted: {checkpoint_id}")
        print(f"üîç DEBUG: Search queries to try: {unique_queries}")
    
    # Collect results from multiple queries
    all_results = []
    seen_content = set()
    
    for query in unique_queries:
        try:
            results = vector_store.similarity_search(query, k=4)
            if verbose:
                print(f"üîç DEBUG: Query '{query}' returned {len(results)} results")
            for doc in results:
                # Deduplicate by content hash (use more of the content for better dedup)
                content_hash = hash(doc.page_content[:500])
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    all_results.append(doc)
        except Exception as e:
            if verbose:
                print(f"üîç DEBUG: Query '{query}' failed: {e}")
    
    # Sort results by relevance to checkpoint_id (if found in content)
    def relevance_score(doc):
        content = doc.page_content.lower()
        score = 0
        if checkpoint_id.lower() in content:
            score += 10
        if "ensure" in content and checkpoint_id.lower() in content:
            score += 5
        if "audit" in content:
            score += 2
        if "remediation" in content:
            score += 2
        return -score  # Negative for descending sort
    
    all_results.sort(key=relevance_score)
    
    # Limit to top 10 unique results
    all_results = all_results[:10]
    
    combined_content = "\n\n---\n\n".join([doc.page_content for doc in all_results])
    
    if verbose:
        print(f"\nüîç DEBUG: Total unique document chunks found: {len(all_results)}")
        print(f"üîç DEBUG: Combined content length: {len(combined_content)} characters")
        print("\n" + "-"*50 + " RAW CONTENT PREVIEW " + "-"*50)
        # Show full content in verbose mode
        print(combined_content[:5000] if len(combined_content) > 5000 else combined_content)
        print("-"*120 + "\n")
    
    # Always show a warning if no content found
    if len(combined_content) < 100:
        print("‚ö†Ô∏è  WARNING: Very little content retrieved from vector store!")
        print("    Make sure CIS_RHEL8_DATA_DEEPSEEK directory has the vector database.")
        print("    You may need to run cis_rhel8_rag_deepseek.py first to create the vector store.")
    
    return combined_content


def get_checkpoint_info_with_ai(checkpoint: str, raw_content: str) -> dict:
    """
    Use DeepSeek AI to extract structured checkpoint information from agent response or raw content.
    
    Args:
        checkpoint: The CIS checkpoint ID/description
        raw_content: Raw content from vector search or agent response
        
    Returns:
        dict: Structured checkpoint information with all fields including audit_procedure and remediation_procedure
    """
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    prompt_template = """You are a CIS RHEL 8 security expert. Analyze the following content and extract structured information about the requested checkpoint.

**Requested Checkpoint:** {checkpoint}

**Content (may be from agent response or raw document chunks):**
{raw_content}

**Task:** 
Carefully read through ALL the content above. The content may be:
- An agent response with structured sections like "### 6. **COMPLETE Audit Procedure**:"
- Raw document chunks separated by "---"
- A mix of both

Look for:
- The checkpoint number (e.g., "1.1.1.1")
- Title starting with "Ensure" or similar
- Sections labeled:
  * "Profile Applicability" or "Profile"
  * "Description"
  * "Rationale" or "Why"
  * "COMPLETE Audit Procedure" or "Audit Procedure" or "Audit" or "How to Audit"
  * "COMPLETE Remediation Procedure" or "Remediation Procedure" or "Remediation" or "How to Remediate"
  * "Impact"
  * "Default Value" or "Default"
  * "References"
- Shell commands in code blocks (```bash or ```shell)
- Commands prefixed with "# " or "```"

**CRITICAL for audit_procedure and remediation_procedure:**
- Look for sections titled "COMPLETE Audit Procedure" or "COMPLETE Remediation Procedure" (these are the most important)
- Include ALL steps, commands, scripts, and code blocks exactly as shown
- Include example outputs if present
- Include verification criteria and requirements
- Preserve all formatting, including bash scripts, code blocks, and multi-line commands
- If you see "Step 1:", "Step 2:", etc., include all steps
- If you see numbered sections like "### 6. **COMPLETE Audit Procedure**:", extract everything until the next numbered section

Extract and return a JSON object with this EXACT structure:
{{
    "checkpoint_id": "The exact checkpoint ID (e.g., 1.1.1.1)",
    "title": "The full title (e.g., 'Ensure cramfs kernel module is not available')",
    "profile_applicability": "Level 1 or Level 2, Server/Workstation",
    "description": "Brief description of what this control does",
    "rationale": "Why this control is important for security",
    "audit_procedure": "The COMPLETE audit procedure including ALL steps, commands, scripts, code blocks, example outputs, and verification criteria. Preserve all formatting.",
    "remediation_procedure": "The COMPLETE remediation procedure including ALL steps, commands, scripts, code blocks, and instructions. Preserve all formatting.",
    "impact": "Any potential impact mentioned",
    "default_value": "The default system value if mentioned",
    "references": "Any CIS or other references",
    "is_dropped": false
}}

**Special Handling:**
- If you see "DROPPED", "DEPRECATED", "REMOVED", "NO LONGER IN", set "is_dropped": true
- For audit_procedure: Include everything from "COMPLETE Audit Procedure" section including all steps, commands, scripts, example outputs, and requirements
- For remediation_procedure: Include everything from "COMPLETE Remediation Procedure" section including all steps, commands, scripts, and instructions
- If a section is not found, use an empty string "" (not "Not found" or "N/A")
- Return ONLY valid JSON, no markdown code blocks, no explanations before or after

Generate the JSON now:"""

    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm
    
    response = chain.invoke({
        'checkpoint': checkpoint,
        'raw_content': raw_content
    })
    
    response_text = response.content.strip()
    
    # Clean up response - remove markdown code blocks if present
    if "```json" in response_text:
        response_text = response_text.split("```json")[1].split("```")[0].strip()
    elif "```" in response_text:
        response_text = response_text.split("```")[1].split("```")[0].strip()
    
    try:
        result = json.loads(response_text)
        
        # Ensure all required fields exist with defaults
        required_fields = {
            'checkpoint_id': checkpoint,
            'title': '',
            'profile_applicability': '',
            'description': '',
            'rationale': '',
            'audit_procedure': '',
            'remediation_procedure': '',
            'impact': '',
            'default_value': '',
            'references': '',
            'is_dropped': False
        }
        
        # Merge with defaults
        for key, default_value in required_fields.items():
            if key not in result:
                result[key] = default_value
        
        # Ensure is_dropped is boolean
        if isinstance(result.get('is_dropped'), str):
            result['is_dropped'] = result['is_dropped'].lower() in ['true', 'yes', '1']
        
        # Always store raw content for fallback use
        result['raw_agent_response'] = raw_content
        
        # Check if AI couldn't find the info - fall back to raw content
        not_found_indicators = ['not found', 'not specified', 'unable to find', 'no information']
        fields_not_found = sum(1 for k, v in result.items() 
                               if k not in ['raw_agent_response', 'is_dropped'] and 
                               isinstance(v, str) and any(ind in v.lower() for ind in not_found_indicators))
        
        if fields_not_found >= 5:  # Most fields not found
            print("‚ö†Ô∏è  AI extraction found limited information. Using raw content as fallback.")
            print("    TIP: Try with full checkpoint description for better results.")
            # Try to extract basic info from raw content directly
            import re
            # Look for checkpoint title pattern
            title_match = re.search(rf'{re.escape(checkpoint)}[^\n]*Ensure[^\n]+', raw_content, re.IGNORECASE)
            if title_match:
                result['title'] = title_match.group(0).strip()
            # Look for audit section
            audit_match = re.search(r'(?:COMPLETE\s+)?Audit(?:\s+Procedure)?[:\s]*\n(.*?)(?=(?:COMPLETE\s+)?Remediation|Impact|Default|References|$)', raw_content, re.DOTALL | re.IGNORECASE)
            if audit_match:
                result['audit_procedure'] = audit_match.group(1).strip()[:2000]
            # Look for remediation section
            remed_match = re.search(r'(?:COMPLETE\s+)?Remediation(?:\s+Procedure)?[:\s]*\n(.*?)(?=Impact|Default|References|$)', raw_content, re.DOTALL | re.IGNORECASE)
            if remed_match:
                result['remediation_procedure'] = remed_match.group(1).strip()[:2000]
        
        return result
    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse JSON response: {e}")
        print(f"Response was: {response_text[:500]}...")
        # Return structure with all required fields
        result = {
            'checkpoint_id': checkpoint,
            'title': 'Unable to parse AI response',
            'profile_applicability': '',
            'description': '',
            'rationale': '',
            'audit_procedure': raw_content[:2000] if raw_content else '',
            'remediation_procedure': raw_content[2000:4000] if len(raw_content) > 2000 else '',
            'impact': '',
            'default_value': '',
            'references': '',
            'is_dropped': False,
            'raw_agent_response': raw_content
        }
        return result


# =============================================================================
# Playbook Requirements Generation (adapted from kcs_to_playbook.py)
# =============================================================================

def extract_audit_steps_from_procedure(audit_procedure: str, checkpoint_id: str = "", title: str = "") -> list:
    """
    Extract individual audit steps/commands/scripts from the CIS audit procedure text.
    
    Carefully parses the audit procedure to extract:
    - Commands to run
    - Expected outputs
    - Pass/fail criteria
    - Cross-referenced verification logic
    
    Args:
        audit_procedure: The complete audit procedure text from CIS benchmark
        checkpoint_id: The checkpoint ID for context
        title: The checkpoint title (e.g., "Ensure freevxfs kernel module is not available")
        
    Returns:
        list: List of requirement strings extracted from the audit procedure, including OVERALL Verify
    """
    import re
    
    requirements = []
    
    if not audit_procedure or len(audit_procedure) < 20:
        return requirements
    
    # Try to extract the module/service name from title or audit procedure
    module_name = ""
    service_name = ""
    
    if title:
        # Extract module name from title like "Ensure freevxfs kernel module is not available"
        module_match = re.search(r'(?:Ensure|Verify)\s+(\w+)\s+(?:kernel\s+)?module', title, re.IGNORECASE)
        if module_match:
            module_name = module_match.group(1)
        
        # Extract service name like "Ensure systemd-timesyncd service is enabled"
        service_match = re.search(r'(?:Ensure|Verify)\s+([a-z0-9_-]+)\s+(?:service|daemon)', title, re.IGNORECASE)
        if service_match:
            service_name = service_match.group(1)
    
    if not module_name:
        # Try to find module name from audit procedure (look for lsmod | grep <module>)
        module_match = re.search(r"lsmod\s*\|\s*grep\s+['\"]?(\w+)['\"]?", audit_procedure, re.IGNORECASE)
        if module_match:
            module_name = module_match.group(1)
    
    # Extract expected outputs from audit procedure
    expected_outputs = {}
    
    # Look for "Example output:" or "expected output:" patterns
    example_output_matches = re.finditer(
        r'(?:Example\s+output|Expected\s+output):\s*```([^`]+)```',
        audit_procedure,
        re.IGNORECASE | re.MULTILINE
    )
    for match in example_output_matches:
        output_text = match.group(1).strip()
        expected_outputs['example'] = output_text
    
    # Look for "Verify output" statements that define pass/fail criteria
    verify_matches = re.finditer(
        r'Verify\s+(?:output\s+)?(?:is|does|contains?|shows?)\s+(?:not\s+)?([^.]+)\.',
        audit_procedure,
        re.IGNORECASE
    )
    verify_criteria = []
    for match in verify_matches:
        verify_criteria.append(match.group(1).strip())
    
    # Pattern 1: Check for audit script existence and add script-based requirement
    has_script = '#!/' in audit_procedure or 'l_output=' in audit_procedure or 'l_mname=' in audit_procedure
    
    if has_script and module_name:
        requirements.append(
            f"Check if the {module_name} kernel module is available on the filesystem using the provided audit script. "
            f"Rationale: PASS (module not available) when the script returns no output or module is built into kernel, "
            f"FAIL (module available) when the script returns a path indicating the module exists as a loadable .ko file."
        )
    elif has_script:
        # Generic script requirement with expected output if available
        rationale = "PASS when script output indicates compliance"
        if 'example' in expected_outputs:
            expected = expected_outputs['example']
            rationale = f"PASS when script output matches expected: '{expected}'"
        if verify_criteria:
            rationale += f", verifying: {verify_criteria[0]}"
        rationale += ", FAIL when script reports non-compliance"
        
        requirements.append(
            f"Run the CIS audit script to check system compliance. "
            f"Rationale: {rationale}."
        )
    
    # Pattern 2: Extract lsmod command for checking loaded modules
    lsmod_match = re.search(r"lsmod\s*\|\s*grep\s+['\"]?(\w+)['\"]?", audit_procedure, re.IGNORECASE)
    if lsmod_match:
        mod = lsmod_match.group(1)
        requirements.append(
            f"Verify the {mod} kernel module is not currently loaded using command: `lsmod | grep '{mod}'`. "
            f"Rationale: PASS (not loaded) when the command returns no output (exit code 1), "
            f"FAIL (loaded) when '{mod}' appears in the output (exit code 0)."
        )
    
    # Pattern 3: Extract modprobe --showconfig command
    modprobe_match = re.search(r"modprobe\s+--showconfig\s*\|\s*grep[^\n]+", audit_procedure, re.IGNORECASE)
    if modprobe_match:
        cmd = modprobe_match.group(0).strip()
        mod = module_name or "the module"
        requirements.append(
            f"Verify the {mod} kernel module is configured to be non-loadable by checking modprobe configuration "
            f"using command: `{cmd}`. "
            f"Rationale: PASS (blacklisted) when output contains 'blacklist {mod}' and 'install {mod} /bin/false' or '/bin/true', "
            f"FAIL (not blacklisted) when these entries are missing."
        )
    
    # Pattern 4: Extract systemctl commands with expected outputs
    systemctl_matches = re.finditer(r"(?:#\s+)?(systemctl\s+(?:is-enabled|status|show)\s+\S+)", audit_procedure, re.IGNORECASE)
    for match in systemctl_matches:
        cmd = match.group(1).strip()
        
        # Build rationale based on command type and expected output
        rationale = ""
        if 'is-enabled' in cmd:
            if 'example' in expected_outputs:
                expected = expected_outputs['example']
                rationale = f"PASS when output matches '{expected}'"
                if verify_criteria:
                    # Add verification criteria (e.g., "not masked or disabled")
                    rationale += f" and {verify_criteria[0]}"
                rationale += f", FAIL otherwise"
            else:
                rationale = "PASS when service is in expected state (enabled/active), FAIL when disabled or masked"
        elif 'status' in cmd:
            rationale = "PASS when service is active/running, FAIL when inactive/stopped"
        else:
            rationale = "PASS when output matches expected configuration, FAIL otherwise"
        
        requirements.append(f"Execute audit command: `{cmd}`. Rationale: {rationale}")
    
    # Pattern 5: Extract findmnt commands for mount point checks
    findmnt_matches = re.finditer(r"(?:#\s+)?(findmnt\s+[^\n]+)", audit_procedure, re.IGNORECASE)
    for match in findmnt_matches:
        cmd = match.group(1).strip()
        
        # Extract mount point from command
        mount_point_match = re.search(r'/\S+', cmd)
        mount_point = mount_point_match.group(0) if mount_point_match else "mount point"
        
        # Build rationale based on expected output
        if 'example' in expected_outputs:
            expected = expected_outputs['example']
            rationale = f"PASS when {mount_point} is mounted with expected options (output contains expected mount info), FAIL when not mounted or incorrect options"
        else:
            rationale = f"PASS when {mount_point} is mounted, FAIL when not mounted"
        
        requirements.append(f"Execute audit command: `{cmd}`. Rationale: {rationale}")
    
    # Pattern 6: Extract grep commands with complex patterns (before generic patterns)
    # These are often the primary audit checks
    grep_matches = re.finditer(
        r'(?:#\s+)?(grep\s+(?:-[A-Za-z]+\s+)*--\s+[^\n]+)',
        audit_procedure,
        re.IGNORECASE
    )
    for match in grep_matches:
        cmd = match.group(1).strip()
        
        # Build rationale based on context around the grep command
        # Look for expected output or verification text near this command
        cmd_pos = match.start()
        context_before = audit_procedure[max(0, cmd_pos-500):cmd_pos]
        context_after = audit_procedure[cmd_pos:min(len(audit_procedure), cmd_pos+500)]
        
        # Check for expected output
        expected_match = re.search(r'(?:expected\s+output|verify.*output|output\s+is)[:\s]*[`"\']?([^`"\'\n]+)[`"\']?', 
                                   context_after, re.IGNORECASE)
        nothing_expected = re.search(r'nothing\s+(?:should\s+be\s+)?returned|no\s+output|empty', 
                                     context_after, re.IGNORECASE)
        
        # Build rationale
        if nothing_expected:
            rationale = f"PASS when command returns no output (nothing found), FAIL when output is returned (non-compliant settings found)"
        elif expected_match:
            expected_val = expected_match.group(1).strip()
            rationale = f"PASS when output shows '{expected_val}' or similar compliant value, FAIL when output is missing or shows non-compliant value"
        elif 'gpgcheck' in cmd:
            rationale = "PASS when gpgcheck is enabled (value=1/true/yes), FAIL when disabled or not set"
        else:
            rationale = "PASS when output matches expected value, FAIL otherwise"
        
        requirements.append(f"Execute audit command: `{cmd}`. Rationale: {rationale}")
    
    # Pattern 7: Extract other specific commands (but skip sed/remediation commands)
    cmd_patterns = [
        (r'sysctl\s+\S+', 'sysctl'),
        (r'cat\s+/etc/\S+', 'cat'),
        (r'stat\s+\S+', 'stat'),
        (r'rpm\s+-q\s+\S+', 'rpm'),
    ]
    
    found_commands = set()
    # Skip systemctl, findmnt, lsmod, modprobe, grep as we already processed them above
    for pattern, cmd_type in cmd_patterns:
        matches = re.findall(pattern, audit_procedure, re.IGNORECASE)
        for cmd in matches:
            cmd = cmd.strip()
            # Skip commands that look like remediation (contain sed, rm, mv, cp, etc.)
            if any(rem_cmd in cmd.lower() for rem_cmd in ['sed -i', ' rm ', ' mv ', ' cp ', 'chmod', 'chown']):
                continue
            if len(cmd) > 5 and cmd not in found_commands:
                found_commands.add(cmd)
                
                if 'sysctl' in cmd:
                    rationale = "PASS when parameter value matches expected setting, FAIL otherwise"
                elif 'rpm' in cmd:
                    rationale = "PASS when package is installed/not installed as required, FAIL otherwise"
                else:
                    rationale = "PASS when output matches expected value, FAIL otherwise"
                
                requirements.append(f"Execute audit command: `{cmd}`. Rationale: {rationale}")
    
    # Pattern 8: Analyze logical relationships and add cross-referenced verify requirements
    # Count how many actual audit commands we have (not including verify statements)
    num_audit_checks = len(requirements)
    
    print(f"    üîç Analyzing {num_audit_checks} audit checks to derive compliance logic...")
    
    # Detect what types of checks we have
    has_findmnt = any('findmnt' in req for req in requirements)
    has_systemctl = any('systemctl' in req for req in requirements)
    has_lsmod = any('lsmod' in req for req in requirements)
    has_modprobe = any('modprobe' in req for req in requirements)
    has_script = any('audit script' in req.lower() for req in requirements)
    
    if module_name and num_audit_checks >= 2:
        # For kernel modules, the compliance logic is:
        # - Module should not be loaded (req_2)
        # - Module should either not exist (req_1) OR be blacklisted (req_3)
        # - Overall PASS: not loaded AND (not available OR blacklisted)
        
        print(f"       üìå Detected kernel module checks for '{module_name}'")
        print(f"       üìå Deriving compliance logic:")
        print(f"          - Module not loaded: req_2")
        print(f"          - Module not available OR blacklisted: (req_1 OR req_3)")
        print(f"          - OVERALL: req_2=PASS AND (req_1=PASS OR req_3=PASS)")
        
        # Add intermediate verification (optional but useful)
        if num_audit_checks >= 3:
            requirements.append(
                f"Verify: the {module_name} kernel module is not available on the system or has been disabled. "
                f"Rationale: PASS when req_1=PASS (module not found on filesystem) OR req_3=PASS (module is blacklisted), "
                f"FAIL when req_1=FAIL AND req_3=FAIL (module exists and not blacklisted)"
            )
            print(f"       ‚úÖ Added intermediate verify (availability/blacklist check)")
        
        # Add OVERALL COMPLIANCE verification
        # This is the final requirement that captures the complete compliance logic
        requirements.append(
            f"OVERALL Verify: the {module_name} kernel module is not loaded and not loadable. "
            f"Rationale: PASS when req_2=PASS (module not loaded) AND (req_1=PASS (module not available) OR req_3=PASS (module blacklisted)), "
            f"FAIL when req_2=FAIL (module is loaded) OR (req_1=FAIL AND req_3=FAIL) (module exists and not blacklisted)"
        )
        print(f"       ‚úÖ Added OVERALL Verify with complete compliance logic")
        
    elif has_findmnt and has_systemctl and num_audit_checks == 2:
        # For mount point + systemd unit checks (e.g., /tmp mount with tmp.mount unit)
        print(f"       üìå Detected mount point + systemd unit checks")
        
        # Extract mount point from title or requirements
        mount_point_match = re.search(r'/\w+', title or ' '.join(requirements))
        mount_point = mount_point_match.group(0) if mount_point_match else "the mount point"
        
        # Extract unit name from requirements
        unit_match = re.search(r'(\S+\.mount)', ' '.join(requirements))
        unit_name = unit_match.group(1) if unit_match else "the systemd unit"
        
        print(f"       üìå Deriving compliance logic:")
        print(f"          - Mount point {mount_point} is mounted: req_1")
        print(f"          - Systemd unit {unit_name} is enabled: req_2")
        print(f"          - OVERALL: req_1=PASS AND req_2=PASS")
        
        requirements.append(
            f"OVERALL Verify: {mount_point} is mounted and systemd will mount it at boot time. "
            f"Rationale: PASS when req_1=PASS ({mount_point} is currently mounted with correct options) AND req_2=PASS ({unit_name} is enabled and not masked or disabled), "
            f"FAIL when mount is not present OR unit is not enabled/masked/disabled"
        )
        print(f"       ‚úÖ Added OVERALL Verify for mount point compliance")
        
    elif service_name and num_audit_checks >= 1:
        # For services, analyze what the checks are verifying
        print(f"       üìå Detected service checks for '{service_name}'")
        
        # Check if we have multiple service state checks
        has_enabled_check = any('is-enabled' in req for req in requirements)
        has_status_check = any('status' in req or 'active' in req for req in requirements)
        
        if has_enabled_check and has_status_check:
            print(f"          - Service must be enabled AND active")
            requirements.append(
                f"OVERALL Verify: the {service_name} service is properly configured and running. "
                f"Rationale: PASS when service is both enabled (req_1=PASS) AND active (req_2=PASS), "
                f"FAIL when service is not enabled OR not active"
            )
        elif has_enabled_check:
            print(f"          - Service must be enabled")
            if 'example' in expected_outputs:
                expected = expected_outputs['example']
                verify_text = f" and {verify_criteria[0]}" if verify_criteria else ""
                requirements.append(
                    f"OVERALL Verify: the {service_name} service is enabled with correct configuration. "
                    f"Rationale: PASS when service state is '{expected}'{verify_text}, "
                    f"FAIL otherwise"
                )
            else:
                requirements.append(
                    f"OVERALL Verify: the {service_name} service is enabled. "
                    f"Rationale: PASS when req_1=PASS (service is enabled), FAIL otherwise"
                )
        else:
            # Generic service check
            requirements.append(
                f"OVERALL Verify: the {service_name} service meets all requirements. "
                f"Rationale: PASS when all service checks pass, FAIL when any check fails"
            )
        print(f"       ‚úÖ Added OVERALL Verify for service compliance")
        
    elif title and num_audit_checks >= 1:
        # Generic checkpoint - derive logic from number of checks
        print(f"       üìå Detected {num_audit_checks} audit check(s)")
        print(f"       üìå Deriving compliance logic from checkpoint title: '{title}'")
        
        # Extract the objective from title
        verify_text = title.replace('Ensure', '').replace('ensure', '').strip()
        
        # Build boolean logic based on number of checks
        if num_audit_checks == 1:
            logic = "PASS when req_1=PASS"
            print(f"          - Single check: {logic}")
            requirements.append(
                f"OVERALL Verify: {verify_text}. "
                f"Rationale: {logic}, FAIL otherwise"
            )
        elif num_audit_checks == 2:
            # Check if it's an OR or AND relationship
            # If checks are alternatives (like "not available OR blacklisted"), use OR
            # Otherwise use AND (most common case)
            if any(keyword in title.lower() for keyword in ['or', 'either', 'alternative']):
                logic = "PASS when req_1=PASS OR req_2=PASS"
                print(f"          - Two checks (OR): {logic}")
                requirements.append(
                    f"OVERALL Verify: {verify_text}. "
                    f"Rationale: {logic}, FAIL when both checks fail"
                )
            else:
                logic = "PASS when req_1=PASS AND req_2=PASS"
                print(f"          - Two checks (AND): {logic}")
                requirements.append(
                    f"OVERALL Verify: {verify_text}. "
                    f"Rationale: {logic}, FAIL when either check fails"
                )
        else:
            # Multiple checks - typically ALL must pass (AND logic)
            req_list = " AND ".join([f"req_{i}=PASS" for i in range(1, num_audit_checks + 1)])
            logic = f"PASS when {req_list}"
            print(f"          - Multiple checks (AND): {logic}")
            requirements.append(
                f"OVERALL Verify: {verify_text}. "
                f"Rationale: {logic}, FAIL when any check fails"
            )
        print(f"       ‚úÖ Added OVERALL Verify with derived compliance logic")
    else:
        print(f"       ‚ö†Ô∏è  Could not derive compliance logic (insufficient checks or context)")
    
    print(f"    ‚úÖ Generated {len(requirements)} total requirements (including OVERALL Verify)")
    
    # Remove duplicates while preserving order
    seen = set()
    unique_requirements = []
    for req in requirements:
        # Create a simplified key for deduplication
        key = re.sub(r'\s+', ' ', req.lower()[:100])
        if key not in seen:
            seen.add(key)
            unique_requirements.append(req)
    
    return unique_requirements


def generate_playbook_requirements_from_checkpoint(checkpoint_info: dict) -> dict:
    """
    Generate playbook requirements based on CIS checkpoint info.
    
    First attempts to extract requirements directly from the audit procedure,
    then uses DeepSeek AI to enhance/format them.
    
    Args:
        checkpoint_info: Dict containing checkpoint details
        
    Returns:
        dict: {
            'objective': str,
            'requirements': list[str]
        }
    """
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    
    checkpoint_id = checkpoint_info.get('checkpoint_id', 'Unknown')
    title = checkpoint_info.get('title', '')
    
    # Get raw agent response if available (this contains the full details from agent search)
    raw_agent_response = checkpoint_info.get('raw_agent_response', '')
    audit_procedure = checkpoint_info.get('audit_procedure', '')
    
    # FIRST: Try to extract requirements directly from audit procedure
    # Use raw_agent_response if audit_procedure is not available (it contains the full audit details)
    source_text = audit_procedure if audit_procedure and len(audit_procedure) > 100 else raw_agent_response
    
    extracted_requirements = extract_audit_steps_from_procedure(source_text, checkpoint_id, title)
    
    if extracted_requirements and len(extracted_requirements) >= 2:
        print(f"    ‚úÖ Extracted {len(extracted_requirements)} audit requirements directly from audit procedure")
        for i, req in enumerate(extracted_requirements[:3], 1):
            preview = req[:100] + '...' if len(req) > 100 else req
            print(f"       {i}. {preview}")
        if len(extracted_requirements) > 3:
            print(f"       ... and {len(extracted_requirements) - 3} more")
        
        return {
            'objective': f"Audit CIS checkpoint {checkpoint_id}: {title}" if title else f"Audit CIS checkpoint: {checkpoint_id}",
            'requirements': extracted_requirements
        }
    
    print(f"    ‚ö†Ô∏è Could not extract requirements directly, using LLM generation...")
    
    # FALLBACK: Use LLM to generate requirements
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    # Build the prompt - include raw agent response if we have it
    additional_context = ""
    if raw_agent_response:
        additional_context = f"""
**Full Agent Response (contains complete checkpoint details):**
{raw_agent_response[:6000]}
"""
    
    prompt_template = """You are an expert system administrator creating Ansible playbooks for CIS benchmark compliance auditing.

Based on the following CIS checkpoint information, generate:
1. A clear playbook objective (one sentence) focused on AUDITING this security control
2. A list of 3-8 specific requirements for an Ansible playbook

**CIS Checkpoint Information:**
- Checkpoint ID: {checkpoint_id}
- Title: {title}
- Profile Applicability: {profile_applicability}
- Description: {description}
- Rationale: {rationale}

**Audit Procedure from CIS Benchmark:**
{audit_procedure}

**Remediation Procedure from CIS Benchmark:**
{remediation_procedure}
{additional_context}
**Task:**
Generate Ansible playbook requirements that will:
1. AUDIT the system to check if it complies with this CIS checkpoint
2. COLLECT the current system state/configuration
3. COMPARE against the expected values from CIS benchmark
4. REPORT compliance status (PASS/FAIL) with details

**Output Format:**
Return ONLY a valid JSON object with this exact structure (no markdown, no code blocks):
{{
    "objective": "Audit CIS checkpoint {checkpoint_id}: {title}",
    "requirements": [
        "Check <condition> using command: `<command>`. Rationale: PASS when <expected result>, FAIL when <failure condition>",
        "Verify <setting> with command: `<command>`. Rationale: PASS when <expected>, FAIL otherwise",
        "Run the following audit script to check <module>: ```#!/usr/bin/env bash\\n<full script content here>```. Rationale: PASS when <condition>, FAIL when <condition>",
        ...
    ]
}}

**Script Inclusion Example:**
If the audit procedure contains a script like:
  #!/usr/bin/env bash
  l_output="" l_output2=""
  ...
You MUST include the ENTIRE script in the requirement, like:
  "Run the following audit script to verify kernel module availability: ```#!/usr/bin/env bash\\nl_output=\"\" l_output2=\"\"\\n...full script...```. Rationale: PASS when script output shows module not available, FAIL otherwise"

**Important Guidelines:**
- Base requirements DIRECTLY on the audit procedure commands from the checkpoint information
- Include the exact commands from the CIS benchmark audit procedure
- Focus on AUDITING (checking compliance), not remediation
- Each requirement should map to a specific audit step
- CRITICAL: Each requirement MUST end with "Rationale: PASS when <condition>, FAIL when <condition>" explaining the pass/fail logic
- Include expected values/outputs for comparison
- If the Full Agent Response is provided, extract the audit commands from there
- CRITICAL: If the audit procedure contains a SCRIPT (bash script, shell script), you MUST include the FULL script content in the requirement, NOT just "use the provided script". The requirement must be SELF-CONTAINED with all script details.
- When including scripts, format them as: "Run the following script: ```<full script content>```"
- Never reference "the audit script" or "the provided script" without including the actual script code

Generate the JSON now:"""

    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm
    
    response = chain.invoke({
        'checkpoint_id': checkpoint_info.get('checkpoint_id', 'Unknown'),
        'title': checkpoint_info.get('title', '') or 'Unknown',
        'profile_applicability': checkpoint_info.get('profile_applicability', '') or 'Not specified',
        'description': checkpoint_info.get('description', '') or 'Not specified',
        'additional_context': additional_context,
        'rationale': checkpoint_info.get('rationale', '') or 'See Full Agent Response above',
        'audit_procedure': checkpoint_info.get('audit_procedure', '') or 'See Full Agent Response above',
        'remediation_procedure': checkpoint_info.get('remediation_procedure', '') or 'See Full Agent Response above'
    })
    
    response_text = response.content.strip()
    
    # Clean up response
    if "```json" in response_text:
        response_text = response_text.split("```json")[1].split("```")[0].strip()
    elif "```" in response_text:
        response_text = response_text.split("```")[1].split("```")[0].strip()
    
    try:
        result = json.loads(response_text)
        return result
    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse JSON response: {e}")
        # Build better fallback requirements using available checkpoint info
        checkpoint_id = checkpoint_info.get('checkpoint_id', 'Unknown')
        title = checkpoint_info.get('title', '')
        audit_proc = checkpoint_info.get('audit_procedure', '')
        raw_response = checkpoint_info.get('raw_agent_response', '')
        
        # Try to extract key audit commands from audit procedure or raw response
        source_text = audit_proc if audit_proc and len(audit_proc) > 50 else raw_response
        fallback_requirements = []
        
        if source_text:
            import re
            # Look for shell commands in the source text
            cmd_patterns = [
                r'(?:Run|Execute|Use)[:\s]+[`\'"]?([^`\'"]+(?:lsmod|modprobe|find|grep|cat|ls|systemctl|sysctl)[^`\'"]*)[`\'"]?',
                r'#\s*([a-z/]+\s+[^\n]+)',  # Commands starting with #
                r'```(?:bash|shell)?\n([^`]+)```',  # Code blocks
            ]
            
            for pattern in cmd_patterns:
                matches = re.findall(pattern, source_text, re.IGNORECASE)
                for match in matches[:5]:  # Limit to 5 commands
                    cmd = match.strip()
                    if len(cmd) > 10 and len(cmd) < 200:
                        fallback_requirements.append(f"Execute audit command: `{cmd}`. Rationale: PASS when command shows compliant state, FAIL otherwise")
        
        # If no commands found, use generic but more specific requirements
        if not fallback_requirements:
            if title:
                fallback_requirements.append(f"Verify that: {title}. Rationale: PASS when verified, FAIL otherwise")
            fallback_requirements.extend([
                f"Check system configuration for CIS {checkpoint_id}. Rationale: PASS when compliant, FAIL otherwise",
                "Collect audit evidence and system state. Rationale: PASS when data collected, FAIL otherwise"
            ])
        
        return {
            'objective': f"Audit CIS checkpoint {checkpoint_id}: {title}" if title else f"Audit CIS checkpoint: {checkpoint_id}",
            'requirements': fallback_requirements
        }


def run_playbook_generation(objective, requirements, target_host, test_host, become_user, filename, skip_execution=True, audit_procedure=None):
    """
    Call langgraph_deepseek_generate_playbook to generate and execute the playbook.
    
    Args:
        objective: Playbook objective
        requirements: List of requirements
        target_host: Target host for execution
        test_host: Test host for validation
        become_user: User to become
        filename: Output filename
        skip_execution: If True, skip playbook execution (NOT USED - kept for compatibility)
        audit_procedure: CIS Benchmark audit procedure (script/commands)
        
    Returns:
        tuple: (success: bool, message: str)
    """
    print("\n" + "="*100)
    print("üöÄ Calling langgraph_deepseek_generate_playbook to generate and execute playbook...")
    print("="*100)
    
    max_retries = max(len(requirements), 3)
    print(f"Max retries: {max_retries} (based on {len(requirements)} requirements)")
    print()
    
    try:
        # Import the workflow function
        from langgraph_deepseek_generate_playbook import generate_playbook_workflow
        
        # Call the workflow programmatically
        final_state = generate_playbook_workflow(
            objective=objective,
            requirements=requirements,
            target_host=target_host,
            test_host=test_host,
            become_user=become_user,
            filename=filename,
            audit_procedure=audit_procedure,
            max_retries=max_retries,
            verbose=True
        )
        
        # Check if successful
        if final_state['workflow_complete'] and final_state['test_success']:
            return True, "Playbook generation and execution completed successfully"
        else:
            return False, f"Workflow failed: {final_state['error_message']}"
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"\n‚ùå Error calling workflow: {str(e)}")
        print(error_details)
        return False, f"Error running playbook generation: {str(e)}"


# =============================================================================
# Interactive Mode
# =============================================================================

def interactive_mode(vector_store, args):
    """Interactive mode to query CIS checkpoints and generate playbooks."""
    print("\n" + "="*70)
    print("CIS RHEL 8 Checkpoint to Ansible Playbook Generator")
    print("="*70)
    print("\nEnter CIS checkpoint IDs to generate audit playbooks.")
    print("Examples:")
    print("  - 1.1.1.1")
    print("  - 1.1.1.1 Ensure cramfs kernel module is not available")
    print("  - 5.2.1 Ensure permissions on /etc/ssh/sshd_config are configured")
    print("\nType 'quit' or 'exit' to stop.\n")
    
    while True:
        try:
            checkpoint = input("Enter checkpoint (or 'quit'): ").strip()
            
            if not checkpoint:
                continue
            
            if checkpoint.lower() in ['quit', 'exit', 'q']:
                print("\nGoodbye!")
                break
            
            process_checkpoint(vector_store, checkpoint, args)
            
        except KeyboardInterrupt:
            print("\n\nInterrupted. Goodbye!")
            break
        except Exception as e:
            print(f"\nError: {e}\n")


def process_checkpoint(vector_store, checkpoint: str, args):
    """Process a single CIS checkpoint and generate playbook."""
    
    verbose = getattr(args, 'verbose', False)
    
    # Step 1 & 2 Combined: Use Agent-based RAG to get checkpoint info (like cis_rhel8_rag_deepseek.py)
    print("\n" + "="*100)
    print(f"STEP 1: Querying CIS RHEL 8 Benchmark using Agent RAG for: '{checkpoint}'")
    print("="*100)
    print("Using agent-based search (same approach as cis_rhel8_rag_deepseek.py)...")
    
    # Use agent to get checkpoint info - this works better than direct search
    agent_response = get_checkpoint_info_with_agent(vector_store, checkpoint, verbose=verbose)
    
    if verbose:
        print(f"\n\nDebug agent_response Begin:\n{agent_response[:2000]}...\n\nDebug agent_response End\n\n")

    # Step 2: Extract structured information from agent response using AI
    print("\n" + "="*100)
    print("STEP 2: Extracting structured checkpoint information from agent response")
    print("="*100)
    
    checkpoint_info = get_checkpoint_info_with_ai(checkpoint, agent_response)
    
    for key, value in checkpoint_info.items():
        print("-"*80)
        print("-"*20, key, "-"*20)
        print(value)
        print("-"*80)
        ok = input("Enter: ")
    
    if verbose:
        print(f"\n‚úÖ Extracted checkpoint information:")
        print(f"   - Checkpoint ID: {checkpoint_info.get('checkpoint_id', 'N/A')}")
        print(f"   - Title: {checkpoint_info.get('title', 'N/A')[:80]}...")
        print(f"   - Audit Procedure length: {len(checkpoint_info.get('audit_procedure', ''))} chars")
        print(f"   - Remediation Procedure length: {len(checkpoint_info.get('remediation_procedure', ''))} chars")

    # Check if checkpoint is DROPPED/DEPRECATED
    if checkpoint_info.get('is_dropped', False):
        print("\n" + "="*100)
        print("‚ö†Ô∏è  CHECKPOINT STATUS: DROPPED/DEPRECATED")
        print("="*100)
        print(f"\n‚ùå Checkpoint {checkpoint_info.get('checkpoint_id', checkpoint)} is no longer in the CIS RHEL 8 Benchmark.")
        print("\nFrom the agent response:")
        # Show relevant part about why it's dropped
        import re
        raw_response = checkpoint_info.get('raw_agent_response', agent_response)
        dropped_section = re.search(
            r'(Status.*?DROPPED.*?)(?:\n\n|\*\*|$)',
            raw_response,
            re.IGNORECASE | re.DOTALL
        )
        if dropped_section:
            print(f"\n{dropped_section.group(1).strip()}")
        
        # Check if the agent mentioned a similar checkpoint
        similar_match = re.search(
            r'(?:similar|related|see|refer to)\s+(?:checkpoint\s+)?(\d+\.\d+\.\d+\.?\d*)',
            raw_response,
            re.IGNORECASE
        )
        
        if similar_match:
            similar_checkpoint = similar_match.group(1)
            print(f"\nüí° Suggestion: The agent mentioned checkpoint {similar_checkpoint} as similar.")
            print(f"   You may want to run: ./cis_checkpoint_to_playbook.py --checkpoint \"{similar_checkpoint}\"")
        
        print("\n" + "="*100)
        print("‚èπÔ∏è  Stopping playbook generation for dropped checkpoint")
        print("="*100)
        return
    
    print(f"\nüìã CIS Checkpoint Details:")
    print("-"*100)
    print(f"ID: {checkpoint_info.get('checkpoint_id', 'N/A')}")
    print(f"Title: {checkpoint_info.get('title', 'N/A') or 'See agent response below'}")
    print(f"Profile: {checkpoint_info.get('profile_applicability', 'N/A') or 'See agent response below'}")
    print(f"\nDescription: {checkpoint_info.get('description', 'N/A') or 'See agent response below'}")
    print(f"\nRationale: {checkpoint_info.get('rationale', 'N/A') or 'See agent response below'}")
    
    # Check if we have parsed audit procedure or need to show raw response
    audit_proc = checkpoint_info.get('audit_procedure', '')
    remed_proc = checkpoint_info.get('remediation_procedure', '')
    raw_response = checkpoint_info.get('raw_agent_response', '')
    
    if audit_proc:
        print("\n" + "-"*100)
        print("üîç AUDIT PROCEDURE:")
        print("-"*100)
        if len(audit_proc) > 2000:
            print(audit_proc[:2000] + "\n... (truncated)")
        else:
            print(audit_proc)
    
    if remed_proc:
        print("\n" + "-"*100)
        print("üîß REMEDIATION PROCEDURE:")
        print("-"*100)
        if len(remed_proc) > 2000:
            print(remed_proc[:2000] + "\n... (truncated)")
        else:
            print(remed_proc)
    
    # If parsing didn't extract key fields, show the full agent response
    if raw_response or (not audit_proc and not remed_proc):
        print("\n" + "-"*100)
        print("üìù FULL AGENT RESPONSE (use this for requirements):")
        print("-"*100)
        if len(raw_response) > 4000:
            print(raw_response[:4000] + "\n... (truncated)")
        else:
            print(raw_response)
    print("-"*100)
    
    # Step 3: Generate playbook requirements
    print("\n" + "="*100)
    print("STEP 3: Generating playbook requirements using DeepSeek AI")
    print("="*100)
    
    playbook_spec = generate_playbook_requirements_from_checkpoint(checkpoint_info)
    
    objective = playbook_spec.get('objective', '')
    requirements = playbook_spec.get('requirements', [])
    
    print(f"\nüìã Generated Playbook Specification:")
    print("-"*100)
    print(f"Objective: {objective}")
    print(f"\nRequirements ({len(requirements)} items):")
    for idx, req in enumerate(requirements, 1):
        print(f"  {idx}. {req}")
    print("-"*100)
    
    # Interactive requirement review (unless --no-interactive)
    if not args.no_interactive:
        print("\n" + "="*100)
        print("üìù REQUIREMENT REVIEW AND FEEDBACK")
        print("="*100)
        print("Options:")
        print("  1. Press ENTER to generate playbook")
        print("  2. Type 'add' to add new requirements")
        print("  3. Type 'edit N' to edit requirement N")
        print("  4. Type 'delete N' to delete requirement N")
        print("  5. Type 'skip' to skip playbook generation")
        print("  6. Type 'help' for more commands")
        print("="*100)
        
        while True:
            user_input = input("\nüë§ Your action (ENTER to generate, 'skip' to skip): ").strip()
            
            if not user_input:
                print("\n‚úÖ Generating playbook...")
                break
            
            if user_input.lower() == 'skip':
                print("\n‚è≠Ô∏è  Skipping playbook generation for this checkpoint")
                return
            
            if user_input.lower() == 'done':
                print("\nüìã Updated Requirements ({} items):".format(len(requirements)))
                for idx, req in enumerate(requirements, 1):
                    print(f"  {idx}. {req}")
                continue
            
            if user_input.lower() == 'add':
                new_req = input("   Enter new requirement: ").strip()
                if new_req:
                    requirements.append(new_req)
                    print(f"   ‚úÖ Added requirement {len(requirements)}: {new_req}")
                continue
            
            if user_input.lower().startswith('edit '):
                try:
                    req_num = int(user_input.split()[1])
                    if 1 <= req_num <= len(requirements):
                        print(f"   Current: {requirements[req_num-1]}")
                        new_text = input("   New text: ").strip()
                        if new_text:
                            requirements[req_num-1] = new_text
                            print(f"   ‚úÖ Updated requirement {req_num}")
                    else:
                        print(f"   ‚ùå Invalid number. Must be 1-{len(requirements)}")
                except (ValueError, IndexError):
                    print("   ‚ùå Invalid format. Use: edit N")
                continue
            
            if user_input.lower().startswith('delete '):
                try:
                    req_num = int(user_input.split()[1])
                    if 1 <= req_num <= len(requirements):
                        deleted = requirements.pop(req_num-1)
                        print(f"   ‚úÖ Deleted: {deleted}")
                    else:
                        print(f"   ‚ùå Invalid number. Must be 1-{len(requirements)}")
                except (ValueError, IndexError):
                    print("   ‚ùå Invalid format. Use: delete N")
                continue
            
            if user_input.lower() == 'help':
                print("\nüìñ Commands: ENTER (generate), skip, add, edit N, delete N, done (show reqs)")
                continue
            
            print("   ‚ùå Unknown command. Type 'help' for options.")
    
    # Add CIS reference to requirements
    checkpoint_id = checkpoint_info.get('checkpoint_id', checkpoint)
    requirements.append(f"Add comment referencing CIS RHEL 8 Benchmark v4.0.0, checkpoint {checkpoint_id}")
    requirements.append(f"""Create a task named 'Generate compliance report' that displays a debug msg with this EXACT format:
========================================================
        COMPLIANCE REPORT - CIS {checkpoint_id}
========================================================
Reference: CIS RHEL 8 Benchmark v4.0.0 checkpoint {checkpoint_id}
========================================================

REQUIREMENT 1 - <requirement description>:
  Task: <task name>
  Command: <command executed>
  Exit code: <exit code>
  Data: <command output>
  Status: PASS or FAIL
  Rationale: <why PASS or FAIL based on the requirement's rationale>

(repeat for each requirement)

========================================================
OVERALL COMPLIANCE:
  Result: PASS or FAIL
  Rationale: <overall pass/fail logic explanation>
========================================================

Each requirement MUST have Status and Rationale lines. The OVERALL COMPLIANCE section is REQUIRED at the end.""")
    requirements.append("CRITICAL: Use ignore_errors: true and failed_when: false on all audit tasks so all checks complete and report status")
    
    # Step 4: Generate playbook filename
    safe_checkpoint_id = checkpoint_id.replace('.', '_').replace(' ', '_')[:20]
    filename = args.filename if args.filename else f"cis_audit_{safe_checkpoint_id}.yml"
    
    # Step 5: Generate and optionally execute playbook
    print("\n" + "="*100)
    print("STEP 4: Generating Ansible Playbook")
    print("="*100)
    print(f"Target Host:    {args.target_host}")
    print(f"Become User:    {args.become_user}")
    print(f"Output File:    {filename}")
    
    # Get the audit procedure from checkpoint info
    # Priority: 1) parsed audit_procedure, 2) raw_agent_response (contains full details)
    audit_procedure = checkpoint_info.get('audit_procedure', '')
    raw_response = checkpoint_info.get('raw_agent_response', '')
    
    # If audit_procedure is empty or generic, try to extract from raw_agent_response
    if not audit_procedure or audit_procedure in ['Not specified', 'See Full Agent Response above', 'N/A', '']:
        if raw_response:
            # Try multiple patterns to extract audit procedure
            import re
            audit_patterns = [
                # Pattern 1: **Audit:** or **Audit Procedure:** section
                r'\*\*Audit(?:\s+Procedure)?\*\*[:\s]*\n?(.*?)(?=\*\*(?:Remediation|Impact|Default|References)|\n\n\*\*|$)',
                # Pattern 2: Audit: or Audit Procedure: section  
                r'(?:Audit|AUDIT)(?:\s+Procedure)?[:\s]*\n(.*?)(?=(?:Remediation|REMEDIATION|Impact|IMPACT|Default|DEFAULT|References|$))',
                # Pattern 3: numbered section like "6. Audit procedure"
                r'\d+\.\s*(?:Audit|AUDIT)[^\n]*\n(.*?)(?=\d+\.\s*(?:Remediation|Impact)|$)',
            ]
            
            for pattern in audit_patterns:
                audit_match = re.search(pattern, raw_response, re.DOTALL | re.IGNORECASE)
                if audit_match and len(audit_match.group(1).strip()) > 50:
                    audit_procedure = audit_match.group(1).strip()
                    break
            
            # If still no audit procedure found, use the FULL raw_agent_response
            # This contains all the checkpoint details including audit commands
            if not audit_procedure or len(audit_procedure) < 50:
                # The raw_agent_response contains the complete checkpoint info
                # which the LLM can use to understand what needs to be audited
                audit_procedure = raw_response
                print(f"    Using full agent response as audit context ({len(audit_procedure)} chars)")
    
    if audit_procedure and len(audit_procedure) > 50:
        print(f"Audit Proc:     {len(audit_procedure)} chars (CIS Benchmark audit procedure will be used)")
        # Show a preview of the audit procedure
        preview_lines = audit_procedure[:500].split('\n')[:5]
        for line in preview_lines:
            if line.strip():
                print(f"                {line[:80]}{'...' if len(line) > 80 else ''}")
        if len(audit_procedure) > 500:
            print(f"                ... ({len(audit_procedure) - 500} more chars)")
    else:
        print(f"Audit Proc:     Not available (using requirements only)")
    
    if args.skip_execution:
        print("‚ö†Ô∏è  Execution will be SKIPPED (--skip-execution flag)")
    
    # audit_procedure was already extracted above in Step 4
    success, output = run_playbook_generation(
        objective=objective,
        requirements=requirements,
        target_host=args.target_host,
        test_host=args.test_host if args.test_host else args.target_host,
        become_user=args.become_user,
        filename=filename,
        skip_execution=args.skip_execution,
        audit_procedure=audit_procedure if audit_procedure and len(audit_procedure) > 50 else None
    )
    
    # Summary
    print("\n" + "="*100)
    print("üìä SUMMARY")
    print("="*100)
    
    if success:
        print(f"‚úÖ Successfully generated audit playbook!")
        print(f"\nüìã CIS Checkpoint: {checkpoint_info.get('checkpoint_id', checkpoint)}")
        print(f"üìÑ Title: {checkpoint_info.get('title', 'N/A')}")
        print(f"üìÅ Playbook: {filename}")
        print(f"üéØ Target: {args.target_host}")
    else:
        print(f"‚ùå Playbook generation failed: {output}")


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Generate Ansible audit playbooks from CIS RHEL 8 checkpoints',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Interactive mode
  python3 cis_checkpoint_to_playbook.py
  
  # Single checkpoint
  python3 cis_checkpoint_to_playbook.py --checkpoint "1.1.1.1"
  
  # With custom target host
  python3 cis_checkpoint_to_playbook.py --checkpoint "1.1.1.1" --target-host 192.168.122.16
  
  # Skip execution (generate only)
  python3 cis_checkpoint_to_playbook.py --checkpoint "5.2.4" --skip-execution
"""
    )
    
    parser.add_argument(
        '--checkpoint', '-c',
        type=str,
        default=None,
        help='CIS checkpoint ID or description (e.g., "1.1.1.1" or "Ensure cramfs kernel module is not available")'
    )
    
    parser.add_argument(
        '--target-host', '-t',
        type=str,
        default='192.168.122.16',
        help='Target host for playbook execution (default: 192.168.122.16)'
    )
    
    parser.add_argument(
        '--test-host',
        type=str,
        default=None,
        help='Test host for validation before target execution'
    )
    
    parser.add_argument(
        '--become-user', '-u',
        type=str,
        default='root',
        help='User to become when executing tasks (default: root)'
    )
    
    parser.add_argument(
        '--filename', '-f',
        type=str,
        default=None,
        help='Output filename for the generated playbook (default: cis_audit_<checkpoint>.yml)'
    )
    
    parser.add_argument(
        '--skip-execution',
        action='store_true',
        help='Generate playbook but skip execution'
    )
    
    parser.add_argument(
        '--no-interactive',
        action='store_true',
        help='Skip interactive requirement review'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Show verbose output including raw search results for debugging'
    )
    
    args = parser.parse_args()
    
    try:
        # Load vector store
        print("\n" + "="*100)
        print("üîß Initializing CIS RHEL 8 Benchmark Vector Store")
        print("="*100)
        
        vector_store = load_or_create_vector_store()
        print("‚úÖ Vector store ready")
        
        if args.checkpoint:
            # Single checkpoint mode
            process_checkpoint(vector_store, args.checkpoint, args)
        else:
            # Interactive mode
            interactive_mode(vector_store, args)
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

